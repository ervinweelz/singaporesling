{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f537cdea",
   "metadata": {},
   "source": [
    "## China opposes UK's 'discriminatory actions' against China-linked deals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7128fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (1.10.9)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: colorama in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\stengg - jing hong\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948f118b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      World\n",
      "  \n",
      "Chinese and British flags fly in London's Chinatown, Britain on Oct 19, 2015. (File Photo: Reuters/Suzanne Plunkett)BEIJING/LONDON: China firmly opposes what it called the British government's \"discriminatory actions\" against Chinese firms, the country's embassy in the UK said after London intervened in Chinese-linked takeovers eight times in the last year.The British government said in a report that it had blocked or imposed conditions on eight transactions involving China-linked investment in domestic companies under its National Security and Investment Act in the last year.\"We strongly urge the British side to stop its unreasonable suppression of Chinese enterprises and provide a fair, just and non-discriminatory business environment for them,\" the Chinese embassy in London said on Wednesday (Jul 12).Introduced in 2021, the National Security and Investment Act aimed to give Britain stronger powers to scrutinise and intervene in foreign investments, having traditionally been one of the most open markets for global mergers and acquisitions.Britain said that China-related investment accounted for eight of the 15 interventions it had made in its first full-year set of figures of how the law was being used.In November Britain ordered Chinese-owned technology company Nexperia to sell at least 86 per cent of Britain's biggest microchip factory, Newport Wafer Fab, following a national security assessment under the law.Britain is attempting something of a reset of relations with China, working together on areas of agreement while balancing national security concerns.Complicating efforts to improve relations is a spat over China's plans to build a new embassy next to the Tower of London, which started as a local dispute but has escalated into a diplomatic standoff.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/world/china-uk-discriminatory-actions-linked-deals-3623826\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "doc = BeautifulSoup(result, \"html.parser\")\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8058bc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      World\n",
      "  \n",
      "Chinese and British flags fly in London's Chinatown, Britain on Oct 19, 2015. (File Photo: Reuters/Suzanne Plunkett)BEIJING/LONDON: China firmly opposes what it called the British government's \"discriminatory actions\" against Chinese firms, the country's embassy in the UK said after London intervened in Chinese-linked takeovers eight times in the last year.The British government said in a report that it had blocked or imposed conditions on eight transactions involving China-linked investment in domestic companies under its National Security and Investment Act in the last year.\"We strongly urge the British side to stop its unreasonable suppression of Chinese enterprises and provide a fair, just and non-discriminatory business environment for them,\" the Chinese embassy in London said on Wednesday (Jul 12).Introduced in 2021, the National Security and Investment Act aimed to give Britain stronger powers to scrutinise and intervene in foreign investments, having traditionally been one of the most open markets for global mergers and acquisitions.Britain said that China-related investment accounted for eight of the 15 interventions it had made in its first full-year set of figures of how the law was being used.In November Britain ordered Chinese-owned technology company Nexperia to sell at least 86 per cent of Britain's biggest microchip factory, Newport Wafer Fab, following a national security assessment under the law.Britain is attempting something of a reset of relations with China, working together on areas of agreement while balancing national security concerns.Complicating efforts to improve relations is a spat over China's plans to build a new embassy next to the Tower of London, which started as a local dispute but has escalated into a diplomatic standoff.\n",
      "['chinese', 'british', 'flag', 'fly', 'firmly', 'opposes', 'called', 'british', 'government', 'discriminatory', 'action', 'chinese', 'firm', 'country', 'embassy', 'said', 'intervened', 'linked', 'takeover', 'time', 'year', 'british', 'government', 'said', 'report', 'blocked', 'imposed', 'condition', 'transaction', 'involving', 'linked', 'investment', 'domestic', 'company', 'year', 'strongly', 'urge', 'british', 'stop', 'unreasonable', 'suppression', 'chinese', 'enterprise', 'provide', 'fair', 'non', 'discriminatory', 'business', 'environment', 'chinese', 'embassy', 'said', 'aimed', 'strong', 'power', 'scrutinise', 'intervene', 'foreign', 'investment', 'traditionally', 'open', 'market', 'global', 'merger', 'acquisition', 'said', 'related', 'investment', 'accounted', 'intervention', 'year', 'set', 'figure', 'law', 'ordered', 'owned', 'technology', 'company', 'sell', 'cent', 'big', 'microchip', 'factory', 'following', 'national', 'security', 'assessment', 'law', 'attempting', 'reset', 'relation', 'working', 'area', 'agreement', 'balancing', 'national', 'security', 'concern', 'Complicating', 'effort', 'improve', 'relation', 'spat', 'plan', 'build', 'new', 'embassy', 'started', 'local', 'dispute', 'escalated', 'diplomatic', 'standoff']\n",
      "Number of unique words: 113\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4feeca5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chinese: 4\n",
      "british: 4\n",
      "said: 4\n",
      "embassy: 3\n",
      "year: 3\n",
      "investment: 3\n",
      "government: 2\n",
      "discriminatory: 2\n",
      "linked: 2\n",
      "company: 2\n",
      "['chinese', 'british', 'said', 'embassy', 'year', 'investment', 'government', 'discriminatory', 'linked', 'company']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation:\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2e290c",
   "metadata": {},
   "source": [
    "## Google AI health chatbot passes US medical exam: Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "817f7769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      Business\n",
      "  \n",
      "The Google logo is pictured at the entrance to the Google offices in London, Britain on Jan 18, 2019. (File photo: REUTERS/Hannah McKay)PARIS: Google's artificial intelligence-powered medical chatbot has achieved a passing grade on a tough United States medical licensing exam, but its answers still fall short of those from human doctors, a peer-reviewed study said on Wednesday (Jul 12).Last year the release of ChatGPT - whose developer OpenAI is backed by Google's rival Microsoft - kicked off a race between tech giants in the burgeoning field of AI.While much has been made about the future possibilities - and dangers - of AI, health is one area where the technology had already shown tangible progress, with algorithms able to read certain medical scans as well as humans.Google first unveiled its AI tool for answering medical questions, called Med-PaLM, in a preprint study in December. Unlike ChatGPT, it has not been released to the public.The US tech giant says Med-PaLM is the first large language model, an AI technique trained on vast amounts of human-produced text, to pass the US Medical Licensing Examination (USMLE).A passing grade for the exam, which is taken by medical students and physicians-in-training in the US, is around 60 per cent.In February, a study said that ChatGPT had achieved passing or near-passing results.In a peer-reviewed study published in the journal Nature on Wednesday, Google researchers said that Med-PaLM had achieved 67.6 per cent on USMLE-style multiple-choice questions.\"Med-PaLM performs encouragingly, but remains inferior to clinicians,\" the study said.To identify and cut down on \"hallucinations\" - the name for when AI models offer up false information - Google said it had developed a new evaluation benchmark.Karan Singhal, a Google researcher and lead author of the new study, told AFP that the team has used the benchmark to test a newer version of their model with \"super exciting\" results.Med-PaLM 2 has reached 86.5 per cent on the USMLE exam, topping the previous version by nearly 20 per cent, according to a preprint study released in May that has not been peer-reviewed.James Davenport, a computer scientist at the United Kingdom's University of Bath not involved in the research, said \"there is an elephant in the room\" for these AI-powered medical chatbots.There is a big difference between answering \"medical questions and actual medicine\", which includes diagnosing and treating genuine health problems,\" he said.Anthony Cohn, an AI expert at the UK's Leeds University, said that hallucinations would likely always be a problem for such large language models, because of their statistical nature.Therefore these models \"should always be regarded as assistants rather than the final decision makers,\" Cohn said.Singhal said that in the future Med-PaLM could be used to support doctors to offer up alternatives that may not have been considered otherwise.The Wall Street Journal reported earlier this week that Med-PaLM 2 has been in testing at the prestigious US Mayo Clinic research hospital since April.Singhal said he could not speak about specific partnerships.But he emphasised that any testing would not be \"clinical, or patient-facing, or are able to cause patients harm\".It would instead be for \"more administrative tasks that can be relatively easily automated, with low stakes\", he added.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/business/google-ai-health-chatbot-passes-us-medical-exam-study-3624546\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "doc = BeautifulSoup(result, \"html.parser\")\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2047c418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Business\n",
      "  \n",
      "The Google logo is pictured at the entrance to the Google offices in London, Britain on Jan 18, 2019. (File photo: REUTERS/Hannah McKay)PARIS: Google's artificial intelligence-powered medical chatbot has achieved a passing grade on a tough United States medical licensing exam, but its answers still fall short of those from human doctors, a peer-reviewed study said on Wednesday (Jul 12).Last year the release of ChatGPT - whose developer OpenAI is backed by Google's rival Microsoft - kicked off a race between tech giants in the burgeoning field of AI.While much has been made about the future possibilities - and dangers - of AI, health is one area where the technology had already shown tangible progress, with algorithms able to read certain medical scans as well as humans.Google first unveiled its AI tool for answering medical questions, called Med-PaLM, in a preprint study in December. Unlike ChatGPT, it has not been released to the public.The US tech giant says Med-PaLM is the first large language model, an AI technique trained on vast amounts of human-produced text, to pass the US Medical Licensing Examination (USMLE).A passing grade for the exam, which is taken by medical students and physicians-in-training in the US, is around 60 per cent.In February, a study said that ChatGPT had achieved passing or near-passing results.In a peer-reviewed study published in the journal Nature on Wednesday, Google researchers said that Med-PaLM had achieved 67.6 per cent on USMLE-style multiple-choice questions.\"Med-PaLM performs encouragingly, but remains inferior to clinicians,\" the study said.To identify and cut down on \"hallucinations\" - the name for when AI models offer up false information - Google said it had developed a new evaluation benchmark.Karan Singhal, a Google researcher and lead author of the new study, told AFP that the team has used the benchmark to test a newer version of their model with \"super exciting\" results.Med-PaLM 2 has reached 86.5 per cent on the USMLE exam, topping the previous version by nearly 20 per cent, according to a preprint study released in May that has not been peer-reviewed.James Davenport, a computer scientist at the United Kingdom's University of Bath not involved in the research, said \"there is an elephant in the room\" for these AI-powered medical chatbots.There is a big difference between answering \"medical questions and actual medicine\", which includes diagnosing and treating genuine health problems,\" he said.Anthony Cohn, an AI expert at the UK's Leeds University, said that hallucinations would likely always be a problem for such large language models, because of their statistical nature.Therefore these models \"should always be regarded as assistants rather than the final decision makers,\" Cohn said.Singhal said that in the future Med-PaLM could be used to support doctors to offer up alternatives that may not have been considered otherwise.The Wall Street Journal reported earlier this week that Med-PaLM 2 has been in testing at the prestigious US Mayo Clinic research hospital since April.Singhal said he could not speak about specific partnerships.But he emphasised that any testing would not be \"clinical, or patient-facing, or are able to cause patients harm\".It would instead be for \"more administrative tasks that can be relatively easily automated, with low stakes\", he added.\n",
      "['business', 'logo', 'pictured', 'entrance', 'office', 'file', 'photo', 'artificial', 'intelligence', 'powered', 'medical', 'chatbot', 'achieved', 'passing', 'grade', 'tough', 'medical', 'licensing', 'exam', 'answer', 'fall', 'short', 'human', 'doctor', 'peer', 'reviewed', 'study', 'said', 'year', 'release', 'chatgpt', 'developer', 'backed', 'rival', 'kicked', 'race', 'tech', 'giant', 'burgeoning', 'field', 'ai.while', 'future', 'possibility', 'danger', 'health', 'area', 'technology', 'shown', 'tangible', 'progress', 'algorithm', 'able', 'read', 'certain', 'medical', 'scan', 'human', 'unveiled', 'tool', 'answering', 'medical', 'question', 'called', 'preprint', 'study', 'chatgpt', 'released', 'public', 'tech', 'giant', 'says', 'large', 'language', 'model', 'technique', 'trained', 'vast', 'amount', 'human', 'produced', 'text', 'pass', 'passing', 'grade', 'exam', 'taken', 'medical', 'student', 'physician', 'training', 'cent', 'study', 'said', 'achieved', 'passing', 'near', 'passing', 'result', 'peer', 'reviewed', 'study', 'published', 'journal', 'researcher', 'said', 'achieved', 'cent', 'usmle', 'style', 'multiple', 'choice', 'question', 'performs', 'encouragingly', 'remains', 'inferior', 'clinician', 'study', 'said', 'identify', 'cut', 'hallucination', 'model', 'offer', 'false', 'information', 'said', 'developed', 'new', 'evaluation', 'benchmark', 'researcher', 'lead', 'author', 'new', 'study', 'told', 'team', 'benchmark', 'test', 'new', 'version', 'model', 'super', 'exciting', 'result', 'med', 'reached', 'cent', 'exam', 'topping', 'previous', 'version', 'nearly', 'cent', 'according', 'preprint', 'study', 'released', 'peer', 'reviewed', 'computer', 'scientist', 'involved', 'research', 'said', 'elephant', 'room', 'powered', 'medical', 'chatbot', 'big', 'difference', 'answering', 'medical', 'question', 'actual', 'medicine', 'includes', 'diagnosing', 'treating', 'genuine', 'health', 'problem', 'said', 'expert', 'said', 'hallucination', 'likely', 'problem', 'large', 'language', 'model', 'statistical', 'nature', 'model', 'regarded', 'assistant', 'final', 'decision', 'maker', 'said', 'said', 'future', 'support', 'doctor', 'offer', 'alternative', 'considered', 'reported', 'early', 'week', 'testing', 'prestigious', 'research', 'hospital', 'said', 'speak', 'specific', 'partnership', 'emphasised', 'testing', 'clinical', 'patient', 'facing', 'able', 'cause', 'patient', 'harm\"', 'instead', 'administrative', 'task', 'relatively', 'easily', 'automated', 'low', 'stake', 'added']\n",
      "Number of unique words: 238\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a5e8904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "medical: 7\n",
      "study: 7\n",
      "model: 5\n",
      "cent: 4\n",
      "exam: 3\n",
      "human: 3\n",
      "peer: 3\n",
      "question: 3\n",
      "new: 3\n",
      "chatbot: 2\n",
      "['medical', 'study', 'model', 'cent', 'exam', 'human', 'peer', 'question', 'new', 'chatbot']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d598e4e",
   "metadata": {},
   "source": [
    "## PROBLEMS - Shopify to launch AI assistant for merchants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bf761d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      Business\n",
      "  \n",
      "FILE PHOTO: The logo of Shopify is seen outside its headquarters in Ottawa, Ontario, Canada, September 28, 2018. REUTERS/Chris Wattie/File Photo  GLOBAL BUSINESS WEEK AHEADCanadian ecommerce firm Shopify said on Wednesday (Jul 12) it would soon launch an artificial intelligence assistant for merchants on its platform, the latest technology company to roll out such a feature.The assistant called \"Sidekick\" would be embedded as a button on Shopify and answer merchant queries, including details about sales trends, CEO Tobi Lutke illustrated in a video posted on Twitter.Companies such as Alibaba Group Holding, Zoom Video Communications and Databricks have all launched AI assistants in the last few months following the massive jump in the use of OpenAI's chatbot ChatGPT.The assistant can also help entrepreneurs update their stores on Shopify. For instance, it can apply discounts on all items on the website, if prompted to do so.The feature is \"coming soon\", the video showed, without specifying the date when it would be rolled out.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/business/shopify-launch-ai-assistant-merchants-3624511\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6b94c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Business\n",
      "  \n",
      "FILE PHOTO: The logo of Shopify is seen outside its headquarters in Ottawa, Ontario, Canada, September 28, 2018. REUTERS/Chris Wattie/File Photo  GLOBAL BUSINESS WEEK AHEADCanadian ecommerce firm Shopify said on Wednesday (Jul 12) it would soon launch an artificial intelligence assistant for merchants on its platform, the latest technology company to roll out such a feature.The assistant called \"Sidekick\" would be embedded as a button on Shopify and answer merchant queries, including details about sales trends, CEO Tobi Lutke illustrated in a video posted on Twitter.Companies such as Alibaba Group Holding, Zoom Video Communications and Databricks have all launched AI assistants in the last few months following the massive jump in the use of OpenAI's chatbot ChatGPT.The assistant can also help entrepreneurs update their stores on Shopify. For instance, it can apply discounts on all items on the website, if prompted to do so.The feature is \"coming soon\", the video showed, without specifying the date when it would be rolled out.\n",
      "['file', 'logo', 'seen', 'headquarters', 'ecommerce', 'firm', 'said', 'soon', 'launch', 'artificial', 'intelligence', 'assistant', 'merchant', 'platform', 'late', 'technology', 'company', 'roll', 'feature', 'assistant', 'called', 'sidekick', 'embedded', 'button', 'answer', 'merchant', 'query', 'including', 'detail', 'sale', 'trend', 'illustrated', 'video', 'posted', 'company', 'launched', 'assistant', 'month', 'following', 'massive', 'jump', 'use', 'chatbot', 'assistant', 'help', 'entrepreneur', 'update', 'store', 'instance', 'apply', 'discount', 'item', 'website', 'prompted', 'feature', 'coming', 'soon', 'video', 'showed', 'specifying', 'date', 'rolled']\n",
      "Number of unique words: 62\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "155a3744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistant: 4\n",
      "soon: 2\n",
      "merchant: 2\n",
      "company: 2\n",
      "feature: 2\n",
      "video: 2\n",
      "file: 1\n",
      "logo: 1\n",
      "headquarters: 1\n",
      "ecommerce: 1\n",
      "['assistant', 'soon', 'merchant', 'company', 'feature', 'video', 'file', 'logo', 'headquarters', 'ecommerce']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82287296",
   "metadata": {},
   "source": [
    "## Elon Musk thinks China is interested in an international AI framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e848fe3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Article content not found.\n",
      "\n",
      "\n",
      "      Business\n",
      "  \n",
      "WASHINGTON :Billionaire Elon Musk said on Wednesday he thinks China is interested in a cooperative international framework on artificial intelligence, from conversations he had when he visited a few weeks ago.Musk made the remarks in a Twitter Space event with two U.S. congressmen, Democrat Ro Khanna and Republican Mike Gallagher.\"China is definitely interested in working in a cooperative international framework for AI regulation,\" Musk said. He added that he had advocated for artificial intelligence regulations and oversight, including in his meetings in China.Musk's remarks came on the day he launched his long-teased artificial intelligence startup, xAI, after arguing for months about AI's potential for \"civilization destruction.\"Musk recently traveled to China and met the foreign, commerce and industry ministers as well as Vice Premier Ding Xuexiang. His Tesla electric car company has a factory in Shanghai.Musk later said the Chinese government would seek to initiate artificial intelligence regulations.On Thursday, China issued a set of interim measures to manage the booming industry, paving the way for its tech companies to roll out AI services.Chinese foreign ministry spokesperson Wang Wenbin said China attached great importance to the development and governance of AI and \"advocates adhering to the principle of human-centred intelligence and creating artificial intelligence for good.\"\"China is willing to enhance communication and exchanges with the international community on AI security governance, promote the establishment of an international mechanism with universal participation, and form a governance framework and standards that share broad consensus,\" Wang told a regular briefing in response to a question about Musk's comments.Several governments are considering how to mitigate the dangers of the emerging technology, which has experienced a boom in investment and consumer popularity in recent months after the release of OpenAI's ChatGPT.Regulators globally have been scrambling to draw up rules governing the use of generative AI, which can create text and images. Its impact has been compared to that of the internet.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/business/elon-musk-thinks-china-interested-international-ai-framework-3625416\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4c82ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "      Business\n",
      "  \n",
      "WASHINGTON :Billionaire Elon Musk said on Wednesday he thinks China is interested in a cooperative international framework on artificial intelligence, from conversations he had when he visited a few weeks ago.Musk made the remarks in a Twitter Space event with two U.S. congressmen, Democrat Ro Khanna and Republican Mike Gallagher.\"China is definitely interested in working in a cooperative international framework for AI regulation,\" Musk said. He added that he had advocated for artificial intelligence regulations and oversight, including in his meetings in China.Musk's remarks came on the day he launched his long-teased artificial intelligence startup, xAI, after arguing for months about AI's potential for \"civilization destruction.\"Musk recently traveled to China and met the foreign, commerce and industry ministers as well as Vice Premier Ding Xuexiang. His Tesla electric car company has a factory in Shanghai.Musk later said the Chinese government would seek to initiate artificial intelligence regulations.On Thursday, China issued a set of interim measures to manage the booming industry, paving the way for its tech companies to roll out AI services.Chinese foreign ministry spokesperson Wang Wenbin said China attached great importance to the development and governance of AI and \"advocates adhering to the principle of human-centred intelligence and creating artificial intelligence for good.\"\"China is willing to enhance communication and exchanges with the international community on AI security governance, promote the establishment of an international mechanism with universal participation, and form a governance framework and standards that share broad consensus,\" Wang told a regular briefing in response to a question about Musk's comments.Several governments are considering how to mitigate the dangers of the emerging technology, which has experienced a boom in investment and consumer popularity in recent months after the release of OpenAI's ChatGPT.Regulators globally have been scrambling to draw up rules governing the use of generative AI, which can create text and images. Its impact has been compared to that of the internet.\n",
      "['said', 'thinks', 'interested', 'cooperative', 'international', 'framework', 'artificial', 'intelligence', 'conversation', 'visited', 'week', 'ago', 'remark', 'event', 'congressman', 'definitely', 'interested', 'working', 'cooperative', 'international', 'framework', 'regulation', 'said', 'added', 'advocated', 'artificial', 'intelligence', 'regulation', 'oversight', 'including', 'meeting', 'remark', 'came', 'day', 'launched', 'long', 'teased', 'artificial', 'intelligence', 'startup', 'arguing', 'month', 'potential', 'civilization', 'destruction', 'recently', 'traveled', 'met', 'foreign', 'commerce', 'industry', 'minister', 'electric', 'car', 'company', 'factory', 'later', 'said', 'chinese', 'government', 'seek', 'initiate', 'artificial', 'intelligence', 'regulation', 'issued', 'set', 'interim', 'measure', 'manage', 'booming', 'industry', 'paving', 'way', 'tech', 'company', 'roll', 'service', 'chinese', 'foreign', 'ministry', 'spokesperson', 'said', 'attached', 'great', 'importance', 'development', 'governance', 'advocate', 'adhering', 'principle', 'human', 'centred', 'intelligence', 'creating', 'artificial', 'intelligence', 'good', 'willing', 'enhance', 'communication', 'exchange', 'international', 'community', 'security', 'governance', 'promote', 'establishment', 'international', 'mechanism', 'universal', 'participation', 'form', 'governance', 'framework', 'standard', 'share', 'broad', 'consensus', 'told', 'regular', 'briefing', 'response', 'question', 'comment', 'government', 'considering', 'mitigate', 'danger', 'emerging', 'technology', 'experienced', 'boom', 'investment', 'consumer', 'popularity', 'recent', 'month', 'release', 'chatgpt.regulator', 'globally', 'scrambling', 'draw', 'rule', 'governing', 'use', 'generative', 'create', 'text', 'image', 'impact', 'compared', 'internet']\n",
      "Number of unique words: 153\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5469113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intelligence: 6\n",
      "artificial: 5\n",
      "international: 4\n",
      "framework: 3\n",
      "regulation: 3\n",
      "governance: 3\n",
      "interested: 2\n",
      "cooperative: 2\n",
      "remark: 2\n",
      "month: 2\n",
      "['intelligence', 'artificial', 'international', 'framework', 'regulation', 'governance', 'interested', 'cooperative', 'remark', 'month']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa00617b",
   "metadata": {},
   "source": [
    "## Commentary: AI isn’t ready to dispel fake news. In fact, it might make things worse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "008b5ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      Commentary\n",
      "  \n",
      "Automated real-time fact-checking is the future, but current AI still has a long way to go towards achieving it, say Joel Skadiang and Jaga Naidu from independent fact-checking platform Black Dot Research.While the idea of automated fact-checking powered by large language models (LLMs) may sound promising, the reality is that we are still a long way from achieving it. (Photo: iStock/Arkadiusz Wargula)SINGAPORE: In an era of rapidly advancing technology, large language models (LLMs) have emerged as powerful tools capable of generating human-like text and transforming various industries. LLMs, a form of artificial intelligence (AI) technology, are computerised language models that have been pre-trained on large textual datasets. With these large databases at their disposal, these language models are capable of generating detailed text based on simple user prompts, capturing much of the syntax and semantics of human language. Although only in their infancy a few years ago, LLMs have progressed by leaps and bounds to become almost omnipresent in present times. Recent developments have created much excitement about the apparently limitless potential of LLMs and generative AI, thanks to user-friendly versions such as ChatGPT that have become exceedingly accessible to the general public. Some observers have raised the possibility of such technology being deployed to combat the rise of disinformation and misinformation, more commonly known as fake news. This could come in the form of assistance to the fact-checking process, which currently still involves significant human intervention.However, there are also growing concerns about the ethical challenges that may arise from automating the fact-checking process, and indeed the dangers of LLMs in being used to generate such fake news themselves.As such, there remains a delicate balance between harnessing the potential of LLMs to assist with fact-checking while acknowledging the limitations that still impede a fully automated implementation.While the idea of automated fact-checking powered by LLMs may sound promising, the reality is that we are still a long way from achieving it. At a panel discussion on navigating the information environment at the Riga StratCom Dialogue 2023, hosted by the NATO Strategic Communications Centre of Excellence, proponents of AI-based fact-checking estimated that it would take between three to five years for fully automated fact-checking to go from being a pipe dream to something that could potentially be implemented. They posited that fact-checking is a complex process that requires human judgment, contextual understanding, and domain expertise, something that is simply not possible with current technological constraints. While LLMs excel at generating text, they lack the ability to comprehend nuance, context, and the subjective nature of truth. As such, they cannot fully discern between fact and opinion or gauge the reliability of sources based on reputation or intent.Last month, two lawyers in the United States were sanctioned after submitting a legal brief written by ChatGPT that included bogus court rulings and citations.\"I simply had no idea that ChatGPT was capable of fabricating entire case citations or judicial opinions, especially in a manner that appeared authentic,\" said one of the lawyers, Steven Schwartz, in a court filing.In addition, LLMs may not always contain up-to-date information. According to OpenAI, the AI research laboratory that launched the widely known ChatGPT, the latest version, GPT-4, has a knowledge cut-off of September 2021. Therefore, one would not be able to rely on LLMs to verify current or developing issues, or respond to emerging misinformation or hostile information campaigns effectively. This limits the potential fact-checking value that LLMs could offer, especially in a time of crisis. Furthermore, the training data used to develop LLMs can introduce biases, reflecting the imperfections of the sources they learn from. This bias can inadvertently perpetuate certain narratives or reinforce existing societal prejudices. LLMs could also easily be exploited by bad actors who develop the models using only biased information to perpetuate their narratives and motives. Automating fact-checking with LLMs would require addressing these biases and ensuring a balanced and unbiased approach, which remains a significant challenge. LLMs have the ability to generate coherent and persuasive text on a wide range of topics, making them valuable resources for content creation. However, this power also opens the door to a potential misuse: The generation of fake news at an alarming rate. LLMs do not operate on an understanding of the language. Rather, LLMs churn out responses based on the information and data that they are provided with. As such, the accuracy of the output is dependent on the accuracy of the input. As LLMs are fed with vast amounts of information, there is a risk that they may incorporate biased or inaccurate data into their responses. This can inadvertently perpetuate false narratives, amplify conspiracy theories, or spread baseless claims that can have detrimental real-world consequences.To illustrate the impact of language models on established fake news-generating tactics, the news-rating group NewsGuard found evidence of Russian state media using AI-generated chatbot screenshots to advance false claims. In a daily news segment posted on Mar 28, 2023, RT reported that “American-made AI-powered search engine ChatGPT lists the 2014 Maidan uprising in Ukraine as a coup that Washington had a hand in, among others. That's in stark contrast to the US narrative that it doesn't interfere in other countries”.Additionally, a report published by NewsGuard last month found an alarming 217 (and counting) AI-generated news and information sites that were operating with little to no human oversight, showcasing the sheer scale of which LLMs can generate false narratives. Should generative AI be deployed for hostile information campaigns from organisations with significant resources at their disposal, then we would certainly be presented with fake news at an unprecedented scale.Despite the dangers, there lies a glimmer of hope that LLMs can be employed as tools to assist fact-checkers in their crucial work. The vast amount of information available to LLMs can be harnessed by independent fact-checkers to cross-reference claims, statements, and articles against known sources of truth. By leveraging the speed and efficiency of these models, fact-checkers can potentially identify and debunk falsehoods with greater accuracy and at a faster rate, helping to combat the rapid spread of fake news in the digital age.As we contemplate the potential role of LLMs in fact-checking, it is crucial to acknowledge the indispensable role of traditional fact-checkers. Human fact-checkers possess critical thinking abilities, subject matter expertise, and the capacity to discern shades of truth. They can delve into complex issues, cross-reference multiple sources, and evaluate the credibility of claims based on contextual understanding.Traditional fact-checkers also have the advantage of being accountable to journalistic ethics and standards. They adhere to rigorous methodologies, follow strict guidelines, and prioritise accuracy and transparency. While LLMs can augment fact-checkers' efforts by assisting in data collection and preliminary analysis, we sincerely believe that the final evaluation and synthesis of information should remain in human hands.In the age of information overload, combating fake news has become an urgent task. LLMs hold immense potential in assisting with fact-checking, but the risks associated with their misuse and the limitations of automation cannot be ignored. While LLMs can aid fact-checkers in processing vast amounts of data and identifying potential falsehoods, they lack the critical thinking abilities, contextual understanding, and domain expertise that human fact-checkers possess.As we move forward, it is crucial to strike a delicate balance between embracing the potential of AI and acknowledging the value of traditional fact-checkers. While we await the inevitable rise of fully automated fact-checking, it remains a reality that traditional fact-checkers still have much to contribute in this arena. Hence, in our current climate, LLMs may be best suited as a tool to empower fact-checking efforts rather than to replace fact-checkers completely. Joel Skadiang and Jaga Naidu are respectively manager and researcher at Black Dot Research, an independent fact-checking platform in Singapore.\n",
      "['Automated', 'real', 'time', 'fact', 'checking', 'future', 'current', 'long', 'way', 'achieving', 'independent', 'fact', 'checking', 'platform', 'idea', 'automated', 'fact', 'checking', 'powered', 'large', 'language', 'model', 'sound', 'promising', 'reality', 'long', 'way', 'achieving', 'photo', 'wargula)singapore', 'era', 'rapidly', 'advancing', 'technology', 'large', 'language', 'model', 'emerged', 'powerful', 'tool', 'capable', 'generating', 'human', 'like', 'text', 'transforming', 'industry', 'llm', 'form', 'artificial', 'intelligence', 'technology', 'computerised', 'language', 'model', 'pre', 'trained', 'large', 'textual', 'dataset', 'large', 'database', 'disposal', 'language', 'model', 'capable', 'generating', 'detailed', 'text', 'based', 'simple', 'user', 'prompt', 'capturing', 'syntax', 'semantic', 'human', 'language', 'infancy', 'year', 'ago', 'llm', 'progressed', 'leap', 'bound', 'omnipresent', 'present', 'time', 'recent', 'development', 'created', 'excitement', 'apparently', 'limitless', 'potential', 'llm', 'generative', 'thank', 'user', 'friendly', 'version', 'exceedingly', 'accessible', 'general', 'public', 'observer', 'raised', 'possibility', 'technology', 'deployed', 'combat', 'rise', 'disinformation', 'misinformation', 'commonly', 'known', 'fake', 'news', 'come', 'form', 'assistance', 'fact', 'checking', 'process', 'currently', 'involves', 'significant', 'human', 'intervention', 'growing', 'concern', 'ethical', 'challenge', 'arise', 'automating', 'fact', 'checking', 'process', 'danger', 'llm', 'generate', 'fake', 'news', 'remains', 'delicate', 'balance', 'harnessing', 'potential', 'llm', 'assist', 'fact', 'checking', 'acknowledging', 'limitation', 'impede', 'fully', 'automated', 'implementation', 'idea', 'automated', 'fact', 'checking', 'powered', 'llm', 'sound', 'promising', 'reality', 'long', 'way', 'achieving', 'panel', 'discussion', 'navigating', 'information', 'environment', 'hosted', 'proponent', 'based', 'fact', 'checking', 'estimated', 'year', 'fully', 'automated', 'fact', 'checking', 'pipe', 'dream', 'potentially', 'implemented', 'posited', 'fact', 'checking', 'complex', 'process', 'requires', 'human', 'judgment', 'contextual', 'understanding', 'domain', 'expertise', 'simply', 'possible', 'current', 'technological', 'constraint', 'llm', 'excel', 'generating', 'text', 'lack', 'ability', 'comprehend', 'nuance', 'context', 'subjective', 'nature', 'truth', 'fully', 'discern', 'fact', 'opinion', 'gauge', 'reliability', 'source', 'based', 'reputation', 'intent', 'month', 'lawyer', 'sanctioned', 'submitting', 'legal', 'brief', 'written', 'included', 'bogus', 'court', 'ruling', 'citation', 'simply', 'idea', 'capable', 'fabricating', 'entire', 'case', 'citation', 'judicial', 'opinion', 'especially', 'manner', 'appeared', 'authentic', 'said', 'lawyer', 'court', 'filing', 'addition', 'llm', 'contain', 'date', 'information', 'According', 'research', 'laboratory', 'launched', 'widely', 'known', 'chatgpt', 'late', 'version', 'knowledge', 'cut', 'able', 'rely', 'llm', 'verify', 'current', 'developing', 'issue', 'respond', 'emerging', 'misinformation', 'hostile', 'information', 'campaign', 'effectively', 'limits', 'potential', 'fact', 'checking', 'value', 'llm', 'offer', 'especially', 'time', 'crisis', 'furthermore', 'training', 'datum', 'develop', 'llm', 'introduce', 'bias', 'reflecting', 'imperfection', 'source', 'learn', 'bias', 'inadvertently', 'perpetuate', 'certain', 'narrative', 'reinforce', 'existing', 'societal', 'prejudice', 'llm', 'easily', 'exploited', 'bad', 'actor', 'develop', 'model', 'biased', 'information', 'perpetuate', 'narrative', 'motive', 'Automating', 'fact', 'checking', 'llm', 'require', 'addressing', 'bias', 'ensuring', 'balanced', 'unbiased', 'approach', 'remains', 'significant', 'challenge', 'llm', 'ability', 'generate', 'coherent', 'persuasive', 'text', 'wide', 'range', 'topic', 'making', 'valuable', 'resource', 'content', 'creation', 'power', 'opens', 'door', 'potential', 'misuse', 'generation', 'fake', 'news', 'alarming', 'rate', 'llm', 'operate', 'understanding', 'language', 'churn', 'response', 'based', 'information', 'datum', 'provided', 'accuracy', 'output', 'dependent', 'accuracy', 'input', 'llm', 'fed', 'vast', 'amount', 'information', 'risk', 'incorporate', 'biased', 'inaccurate', 'datum', 'response', 'inadvertently', 'perpetuate', 'false', 'narrative', 'amplify', 'conspiracy', 'theory', 'spread', 'baseless', 'claim', 'detrimental', 'real', 'world', 'consequence', 'illustrate', 'impact', 'language', 'model', 'established', 'fake', 'news', 'generating', 'tactic', 'news', 'rating', 'group', 'found', 'evidence', 'russian', 'state', 'medium', 'generated', 'chatbot', 'screenshot', 'advance', 'false', 'claim', 'daily', 'news', 'segment', 'posted', 'reported', 'powered', 'search', 'engine', 'chatgpt', 'lists', 'uprising', 'coup', 'hand', 'stark', 'contrast', 'narrative', 'interfere', 'countries”', 'additionally', 'report', 'published', 'month', 'found', 'alarming', 'counting', 'generated', 'news', 'information', 'site', 'operating', 'little', 'human', 'oversight', 'showcasing', 'sheer', 'scale', 'llm', 'generate', 'false', 'narrative', 'generative', 'deployed', 'hostile', 'information', 'campaign', 'organisation', 'significant', 'resource', 'disposal', 'certainly', 'presented', 'fake', 'news', 'unprecedented', 'scale', 'danger', 'lies', 'glimmer', 'hope', 'llm', 'employed', 'tool', 'assist', 'fact', 'checker', 'crucial', 'work', 'vast', 'information', 'available', 'llm', 'harnessed', 'independent', 'fact', 'checker', 'cross', 'reference', 'claim', 'statement', 'article', 'known', 'source', 'truth', 'leveraging', 'speed', 'efficiency', 'model', 'fact', 'checker', 'potentially', 'identify', 'debunk', 'falsehood', 'great', 'accuracy', 'fast', 'rate', 'helping', 'combat', 'rapid', 'spread', 'fake', 'news', 'digital', 'age', 'contemplate', 'potential', 'role', 'llm', 'fact', 'checking', 'crucial', 'acknowledge', 'indispensable', 'role', 'traditional', 'fact', 'checker', 'human', 'fact', 'checker', 'possess', 'critical', 'thinking', 'ability', 'subject', 'matter', 'expertise', 'capacity', 'discern', 'shade', 'truth', 'delve', 'complex', 'issue', 'cross', 'reference', 'multiple', 'source', 'evaluate', 'credibility', 'claim', 'based', 'contextual', 'understanding', 'traditional', 'fact', 'checker', 'advantage', 'accountable', 'journalistic', 'ethic', 'standard', 'adhere', 'rigorous', 'methodology', 'follow', 'strict', 'guideline', 'prioritise', 'accuracy', 'transparency', 'llm', 'augment', 'fact', 'checker', 'effort', 'assisting', 'datum', 'collection', 'preliminary', 'analysis', 'sincerely', 'believe', 'final', 'evaluation', 'synthesis', 'information', 'remain', 'human', 'hand', 'age', 'information', 'overload', 'combating', 'fake', 'news', 'urgent', 'task', 'llm', 'hold', 'immense', 'potential', 'assisting', 'fact', 'checking', 'risk', 'associated', 'misuse', 'limitation', 'automation', 'ignored', 'llm', 'aid', 'fact', 'checker', 'processing', 'vast', 'amount', 'datum', 'identifying', 'potential', 'falsehood', 'lack', 'critical', 'thinking', 'ability', 'contextual', 'understanding', 'domain', 'expertise', 'human', 'fact', 'checker', 'possess', 'forward', 'crucial', 'strike', 'delicate', 'balance', 'embracing', 'potential', 'acknowledging', 'value', 'traditional', 'fact', 'checker', 'await', 'inevitable', 'rise', 'fully', 'automated', 'fact', 'checking', 'remains', 'reality', 'traditional', 'fact', 'checker', 'contribute', 'arena', 'current', 'climate', 'llm', 'well', 'suited', 'tool', 'empower', 'fact', 'checking', 'effort', 'replace', 'fact', 'checker', 'completely', 'respectively', 'manager', 'researcher', 'independent', 'fact', 'checking', 'platform']\n",
      "Number of unique words: 703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact: 30\n",
      "llm: 24\n",
      "checker: 12\n",
      "checking: 11\n",
      "information: 11\n",
      "news: 10\n",
      "human: 8\n",
      "potential: 8\n",
      "language: 7\n",
      "model: 7\n",
      "['fact', 'llm', 'checker', 'checking', 'information', 'news', 'human', 'potential', 'language', 'model']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/commentary/ai-chatgpt-large-language-models-fake-news-fact-checking-3623186\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6c2575",
   "metadata": {},
   "source": [
    "## AI explosion merits regulation to rein in threats, experts say"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8770d916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Article content not found.\n",
      "Article content not found.\n",
      "\n",
      "\n",
      "      Business\n",
      "  \n",
      "AUSTIN :     Rapid advancements in artificial intelligence have the potential to exacerbate societal problems and even pose an existential threat to human life, increasing the need for global regulation, AI experts told the Reuters MOMENTUM conference this week. The explosion of generative AI - which can create text, photos and videos in response to open-ended prompts - in recent months has spurred both excitement about its potential as well as fears it could make some jobs obsolete, upend economies and even possibly overpower humans.\"We are flying down the highway in this car of AI,\" said Ian Swanson, CEO and co-founder of Protect AI, which helps businesses secure their AI and machine learning systems, during a Reuters MOMENTUM panel on Tuesday. \"So what do we need to do? We need to have safety checks. We need to do the proper basic maintenance and we need regulation.\"Regulators need look no further than at social media platforms to understand how unchecked growth of a new industry can lead to negative consequences like creating an information echo chamber, said Seth Dobrin, CEO of Trustwise.\"If we expand the digital divide ... that's going to lead to disruption of society,\" Dobrin said. \"Regulators need to think about that.\"Regulation is already being prepared in several countries to tackle issues around AI.The European Union's proposed AI Act, for example, would classify AI applications into different risk levels, banning uses considered \"unacceptable\" and subjecting \"high-risk\" applications to rigorous assessments.U.S. lawmakers last month introduced two separate AI-focused bills, one that would require the U.S. government to be transparent when using AI to interact with people and another that would establish an office to determine if the United States remains competitive in the latest technologies.One emerging threat that lawmakers and tech leaders must guard against is the possibility of AI making nuclear weapons even more powerful, Anthony Aguirre, founder and executive director of the Future of Life Institute, said in an interview at the conference.Developing ever-more powerful AI will also risk eliminating jobs to a point where it may be impossible for humans to simply learn new skills and enter other industries. \"We're going to end up in a world where our skills are irrelevant,\" he said. The Future of Life Institute, a nonprofit aimed at reducing catastrophic risks from advanced artificial intelligence, made headlines in March when it released an open letter calling for a six-month pause on the training of AI systems more powerful than OpenAI's GPT-4. It warned that AI labs have been \"locked in an out-of-control race\" to develop \"powerful digital minds that no one – not even their creators – can understand, predict, or reliably control.\"\"It seems like the most obvious thing in the world not to put AI into nuclear command and control,\" he said. \"That doesn't mean we won't do that, because we do a lot of unwise things.\"(This story has been corrected to say Seth Dobrin is CEO of Trustwise, not president of the Responsible AI Institute, in paragraph 5)\n",
      "['rapid', 'advancement', 'artificial', 'intelligence', 'potential', 'exacerbate', 'societal', 'problem', 'pose', 'existential', 'threat', 'human', 'life', 'increasing', 'need', 'global', 'regulation', 'expert', 'told', 'conference', 'week', 'explosion', 'generative', 'create', 'text', 'photo', 'video', 'response', 'open', 'ended', 'prompt', 'recent', 'month', 'spurred', 'excitement', 'potential', 'fear', 'job', 'obsolete', 'upend', 'economy', 'possibly', 'overpower', 'human', 'flying', 'highway', 'car', 'said', 'co', 'founder', 'helps', 'business', 'secure', 'machine', 'learning', 'system', 'panel', 'need', 'need', 'safety', 'check', 'need', 'proper', 'basic', 'maintenance', 'need', 'regulation', '\"regulator', 'need', 'look', 'social', 'medium', 'platform', 'understand', 'unchecked', 'growth', 'new', 'industry', 'lead', 'negative', 'consequence', 'creating', 'information', 'said', 'ceo', '\"If', 'expand', 'digital', 'divide', 'going', 'lead', 'disruption', 'society', 'said', 'regulator', 'need', 'think', '\"regulation', 'prepared', 'country', 'tackle', 'issue', 'proposed', 'example', 'classify', 'application', 'different', 'risk', 'level', 'banning', 'use', 'considered', 'unacceptable', 'subjecting', 'high', 'risk', 'application', 'rigorous', 'assessment', 'lawmaker', 'month', 'introduced', 'separate', 'focused', 'bill', 'require', 'government', 'transparent', 'interact', 'people', 'establish', 'office', 'determine', 'remains', 'competitive', 'late', 'technology', 'emerging', 'threat', 'lawmaker', 'tech', 'leader', 'guard', 'possibility', 'making', 'nuclear', 'weapon', 'powerful', 'founder', 'executive', 'director', 'said', 'interview', 'conference', 'Developing', 'powerful', 'risk', 'eliminating', 'job', 'point', 'impossible', 'human', 'simply', 'learn', 'new', 'skill', 'enter', 'industry', 'going', 'end', 'world', 'skill', 'irrelevant', 'said', 'nonprofit', 'aimed', 'reducing', 'catastrophic', 'risk', 'advanced', 'artificial', 'intelligence', 'headline', 'released', 'open', 'letter', 'calling', 'month', 'pause', 'training', 'system', 'powerful', 'warned', 'lab', 'locked', 'control', 'race', 'develop', 'powerful', 'digital', 'mind', 'creator', 'understand', 'predict', 'reliably', 'control', 'obvious', 'thing', 'world', 'nuclear', 'command', 'control', 'said', 'mean', 'lot', 'unwise', 'thing', 'story', 'corrected', 'ceo', 'president', 'paragraph']\n",
      "Number of unique words: 222\n",
      "powerful: 4\n",
      "human: 3\n",
      "month: 3\n",
      "risk: 3\n",
      "control: 3\n",
      "artificial: 2\n",
      "intelligence: 2\n",
      "potential: 2\n",
      "threat: 2\n",
      "regulation: 2\n",
      "['powerful', 'human', 'month', 'risk', 'control', 'artificial', 'intelligence', 'potential', 'threat', 'regulation']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/business/ai-explosion-merits-regulation-rein-threats-experts-say-3625266\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d96a996",
   "metadata": {},
   "source": [
    "## NATO still divided over Ukraine's membership path, despite agreeing to pull country closer: Analysts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1a20179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      World\n",
      "  \n",
      "Concerns over getting pulled into direct conflict with Russia has left some NATO countries reluctant to let Ukraine in too soon.Ukraine's President Volodymyr Zelenskyy arrives for an event on the sidelines of a NATO summit in Vilnius, Lithuania, on Jul 11, 2023. (Photo: AP/Pavel Golovkin)The lack of a clear pathway for Ukraine into the North Atlantic Treaty Organization (NATO) shows that member states are still divided over how soon the country should join the alliance, despite vague assurances of its future membership, said observers. Concerns over getting pulled into direct conflict with Russia has left some NATO countries reluctant to let Ukraine in too soon.“NATO is a consensus organisation. So any agreement that comes out of the alliance needs to be made at consensus. It can't be a consensus minus one (member),” said Ms Rachel Rizzo, senior fellow at the Atlantic Council’s Europe Center. This means all 31 NATO nations must approve of Ukraine’s membership. “There's some question about the willingness of certain members to really grant Ukraine a clear pathway to NATO membership,” Ms Rizzo added. NATO leaders on Tuesday (Jul 11) agreed that Ukraine's future lies within the alliance, but stopped short of offering Kyiv an immediate invitation or timeline for accession that the country has been seeking. “We will be in a position to extend an invitation to Ukraine to join the alliance when allies agree and conditions are met,” said NATO in a declaration at a summit in Lithuanian capital Vilnius, without specifying the conditions Ukraine needs to meet. Its members in eastern Europe have supported Kyiv's push for a swift entry, but countries such as the United States and Germany have been more cautious of a move that could draw NATO into direct conflict with Russia.“The US is still nervous about escalation, and what this might mean for future relationship with Russia,” said Ms Rizzo, whose research focuses on European security, NATO, and the transatlantic relationship. “They want to avoid getting pulled into the war, of course.”The difficulties lie in how a collective defence clause remains a cornerstone of the alliance, said Ms Rizzo, highlighting Article 5 of the North Atlantic Treaty, which states an attack on one member of NATO is an attack against all members.“It basically requires, by treaty, NATO countries to come to the aid of another country should they be invaded.”While NATO has dropped the requirement for Ukraine to fulfil a so-called Membership Action Plan (MAP), removing a hurdle on Kyiv's way into the alliance, observers are not optimistic of Ukraine’s entry into the bloc any time soon. “Unfortunately, I don't see a clear pathway right now where the US would be on board for Ukraine's NATO membership or for offering extremely clear language of how to lay that out,” Ms Rizzo told CNA’s Asia Tonight on Tuesday. “The question I always come back to is, if (Ukraine was) a member of NATO, would we come to the defence of Ukraine? And if we didn't, what would that mean for the entire NATO security architecture?“I think NATO leaders or US leaders ask themselves that same question, which still seems like it would be a really, really difficult proposition, even five or 10 years down the line.”Speaking at a rally in Vilnius on Tuesday, Ukrainian President Volodymyr Zelenskyy voiced his disappointment that Ukraine was not invited to join the alliance. He had been pushing for his country to become a member of the bloc since Russia’s invasion more than a year ago. “There is no such thing as de facto NATO membership that President Zelenskyy is referring to,” said Dr Sara Bjerg Moller, senior fellow in the Indo-Pacific Security Initiative at the Atlantic Council’s Scowcroft Center for Strategy and Security.  “You're either in the military alliance or you're not, and the fact is that Ukraine is not currently a member.”Dr Moller, whose areas of expertise include transatlantic and Indo-Pacific defence and security partnerships, said while the aid that NATO provides Ukraine will increase, it has to “seriously weigh” the possibility of letting the country join. “The reality is that NATO simply cannot invite a country that is currently at war - and at war with another country that happens to have the largest nuclear arsenal in the world - into the military lines,” she told CNA938 on Wednesday. “At the end of the day, it is often the US that pulls and has the most sway.”Doing away with the requirement of a Membership Action Plan could potentially shorten the timeline for Ukraine, suggested Dr Moller, who is also an associate teaching professor in the security studies programme at Georgetown University. However, there are other conditions that Ukraine would still have to meet, “not just in terms of preparing its military and defence operations, so that they are interoperable and standardised with NATO equipment, but also in terms of governance issues and upholding reforms that have already started”, she added.At the Vilnius summit on Tuesday, NATO members committed to reinforce the alliance’s collective deterrence and defence.Its new NATO defence plans, designed to counter its two main threats of Russia and terrorism, “will revolutionise the alliance's ability to do collective defence,” said Dr Moller.“It’s really the first time since the Cold War that there are going to be standing defence plans for how to protect every inch of NATO territory against not just a Russian attack, but also the threat some countries in the alliance fear from terrorism.”With NATO making progress on several fronts after the Russia-Ukraine war started last year, “there's only one clear loser here and that's (Russian President Vladimir) Putin”, she noted. On Moscow’s justification that its invasion of Ukraine was necessary to prevent NATO from expanding, Dr Moller said: “They've gotten that through their actions, so they brought about the thing they claimed they were hoping to prevent.”\n",
      "['pulled', 'direct', 'conflict', 'left', 'country', 'reluctant', 'let', 'soon', 'arrives', 'event', 'sideline', 'summit', 'photo', 'lack', 'clear', 'pathway', 'shows', 'member', 'state', 'divided', 'soon', 'country', 'join', 'alliance', 'vague', 'assurance', 'future', 'membership', 'said', 'observer', 'concern', 'pulled', 'direct', 'conflict', 'left', 'country', 'reluctant', 'let', 'soon', 'consensus', 'organisation', 'agreement', 'comes', 'alliance', 'needs', 'consensus', 'consensus', 'member', 'said', 'senior', 'fellow', 'means', 'nation', 'approve', 'membership', 'question', 'willingness', 'certain', 'member', 'grant', 'clear', 'pathway', 'membership', 'added', 'leader', 'agreed', 'future', 'lies', 'alliance', 'stopped', 'short', 'offering', 'immediate', 'invitation', 'timeline', 'accession', 'country', 'seeking', 'position', 'extend', 'invitation', 'join', 'alliance', 'ally', 'agree', 'condition', 'met', 'said', 'declaration', 'summit', 'lithuanian', 'capital', 'specifying', 'condition', 'needs', 'meet', 'member', 'eastern', 'supported', 'push', 'swift', 'entry', 'country', 'cautious', 'draw', 'direct', 'conflict', 'nervous', 'escalation', 'mean', 'future', 'relationship', 'said', 'research', 'focuses', 'european', 'security', 'transatlantic', 'relationship', 'want', 'avoid', 'pulled', 'war', 'course', 'difficulty', 'lie', 'collective', 'defence', 'clause', 'remains', 'cornerstone', 'alliance', 'said', 'highlighting', 'states', 'attack', 'member', 'attack', 'member', 'basically', 'requires', 'treaty', 'country', 'come', 'aid', 'country', 'invaded', 'dropped', 'requirement', 'fulfil', 'called', 'removing', 'hurdle', 'way', 'alliance', 'observer', 'optimistic', 'entry', 'bloc', 'time', 'soon', 'unfortunately', 'clear', 'pathway', 'right', 'board', 'membership', 'offering', 'extremely', 'clear', 'language', 'lay', 'told', 'tonight', 'question', 'come', 'member', 'come', 'defence', 'mean', 'entire', 'security', 'architecture?“i', 'think', 'leader', 'leader', 'ask', 'question', 'difficult', 'proposition', 'year', 'line', '”Speaking', 'rally', 'ukrainian', 'voiced', 'disappointment', 'invited', 'join', 'alliance', 'pushing', 'country', 'member', 'invasion', 'year', 'ago', 'thing', 'membership', 'referring', 'said', 'senior', 'fellow', 'military', 'alliance', 'fact', 'currently', 'member', 'area', 'expertise', 'include', 'transatlantic', 'defence', 'security', 'partnership', 'said', 'aid', 'provides', 'increase', 'seriously', 'weigh', 'possibility', 'letting', 'country', 'join', 'reality', 'simply', 'invite', 'country', 'currently', 'war', 'war', 'country', 'happens', 'large', 'nuclear', 'arsenal', 'world', 'military', 'line', 'told', 'end', 'day', 'pulls', 'sway', '”Doing', 'away', 'requirement', 'potentially', 'shorten', 'timeline', 'suggested', 'associate', 'teaching', 'professor', 'security', 'study', 'programme', 'condition', 'meet', 'term', 'preparing', 'military', 'defence', 'operation', 'interoperable', 'standardised', 'equipment', 'term', 'governance', 'issue', 'upholding', 'reform', 'started', 'added', 'summit', 'member', 'committed', 'reinforce', 'alliance', 'collective', 'deterrence', 'defence', 'new', 'defence', 'plan', 'designed', 'counter', 'main', 'threat', 'terrorism', 'revolutionise', 'alliance', 'ability', 'collective', 'defence', 'said', 'time', 'going', 'standing', 'defence', 'plan', 'protect', 'inch', 'territory', 'russian', 'attack', 'threat', 'country', 'alliance', 'fear', 'terrorism', 'making', 'progress', 'front', 'war', 'started', 'year', 'clear', 'loser', 'russian', 'noted', 'justification', 'invasion', 'necessary', 'prevent', 'expanding', 'said', 'gotten', 'action', 'brought', 'thing', 'claimed', 'hoping', 'prevent']\n",
      "Number of unique words: 344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "country: 12\n",
      "alliance: 11\n",
      "member: 10\n",
      "defence: 8\n",
      "clear: 5\n",
      "membership: 5\n",
      "soon: 4\n",
      "security: 4\n",
      "war: 4\n",
      "direct: 3\n",
      "['country', 'alliance', 'member', 'defence', 'clear', 'membership', 'soon', 'security', 'war', 'direct']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/world/why-dont-some-nato-countries-want-ukraine-join-3623341\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39432513",
   "metadata": {},
   "source": [
    "## More submarines, jets for Indian navy on cards as Modi visits France"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8c99102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      Asia\n",
      "  \n",
      "FILE PHOTO: French President Emmanuel Macron meets Indian Prime Minister Narendra Modi at the Elysee Palace in Paris, France, on May 4, 2022. (Photo: Francois Mori/Pool via REUTERS)NEW DELHI/PARIS: Indian Prime Minister Narendra Modi heads to France on Thursday (Jul 13) to deepen ties with New Delhi's oldest strategic partner in the West, with a slew of high-profile defence deals expected and a new joint plan to ensure stability in the Indo-Pacific.Modi has been invited as the guest of honour at the Bastille Day celebrations by French President Emmanuel Macron. Units from India's army, navy and air force will also participate in the parade, including two of the 36 Rafale fighter jets India bought in 2015 for nearly US$9 billion.The visit will \"provide an opportunity to chart the course of the partnership for the future across diverse sectors such as strategic, cultural, scientific, academic and economic cooperation\", the Indian government said in a statement on Wednesday.This year marks 25th anniversary of the strategic partnership between the two countries, and the announcement of the new military deals will burnish the deepening defence ties between the two nations.France has been one of India’s closest partners in Europe for decades. Paris was the only Western capital to not impose sanctions on New Delhi after India conducted nuclear tests in 1998.Ten years later, when India got Nuclear Suppliers Group waiver for its civil nuclear plans, France was the first country to sign an agreement.India has relied on French fighter jets for four decades now. Much before buying Dassault Aviation's Rafale in 2015, India bought Mirage jets in 1980s, which still comprise two squadrons of the air force.In 2005, India bought six Scorpene-class diesel submarines from France for 188 billion rupees (US$2.28 billion) to be built in India by Mazagon Dock Shipbuilders (MDL) in partnership with the French Naval Group, the last of which will be commissioned next year.The ageing fleet of India's Russian-made platforms, Moscow’s inability to perform maintenance work, and delays in India's indigenous manufacturing plans for parallel platforms have necessitated the two new defence deals.For submarines, India is expected to buy three more Scorpene submarines, which will again be made by MDL and the Naval group, government sources in New Delhi and Paris said. The price of the deals has yet to be agreed.India is also expected to agree to buy 26 Rafale jets, the sources said, without giving the deal's expected value.The marine version of Dassualt's Rafale jets, intended for India's first indigenous aircraft carrier commissioned in August 2022, outperformed the American SuperhornetF18s in tests last year for Indian requirements.During the visit, Macron will host Modi for a private dinner, as well as a state banquet at the Louvre Museum. Modi will also meet other political leaders, selected French personalities and business leaders, and interact with the Indian diaspora.Both India and France through its island territories have deep interests in the Indian Ocean and are concerned about China's growing assertiveness in the region. Details of an announcement on a plan for the region are not known.The visit comes less than a month after President Joe Biden hosted Modi for a state visit, during which the U.S. offered critical military technology including fighter jet engines and high-altitude drones to India.Last week Modi chaired an online meeting of leaders of the Shanghai Cooperation Group members, which included Russian President Vladimir Putin and China's Xi Jinping.Macron, Biden, Putin and Xi, along with leaders of the other G20 members countries are expected to visit New Delhi in September for its summit to be hosted by India as its head this year.The Indian leader will also later visit the UAE and meet President Sheikh Mohamed bin Zayed Al Nahyan, who is also the ruler of Abu Dhabi.\n",
      "['file', 'photo', 'french', 'meets', 'indian', 'photo', 'indian', 'heads', 'deepen', 'tie', 'old', 'strategic', 'partner', 'slew', 'high', 'profile', 'defence', 'deal', 'expected', 'new', 'joint', 'plan', 'ensure', 'stability', 'invited', 'guest', 'honour', 'celebration', 'french', 'unit', 'army', 'air', 'force', 'participate', 'parade', 'including', 'fighter', 'jet', 'bought', 'nearly', 'visit', 'provide', 'opportunity', 'chart', 'course', 'partnership', 'future', 'diverse', 'sector', 'strategic', 'cultural', 'scientific', 'academic', 'economic', 'cooperation', 'indian', 'government', 'said', 'statement', 'year', 'marks', '25th', 'anniversary', 'strategic', 'partnership', 'country', 'announcement', 'new', 'military', 'deal', 'burnish', 'deepening', 'defence', 'tie', 'nation', 'close', 'partner', 'decade', 'western', 'capital', 'impose', 'sanction', 'conducted', 'nuclear', 'test', 'year', 'later', 'got', 'waiver', 'civil', 'nuclear', 'plan', 'country', 'sign', 'agreement', 'relied', 'french', 'fighter', 'jet', 'decade', 'buying', 'bought', 'jet', 'comprise', 'squadron', 'air', 'force', 'bought', 'scorpene', 'class', 'diesel', 'submarine', 'rupee', 'built', 'partnership', 'commissioned', 'year', 'ageing', 'fleet', 'platform', 'inability', 'perform', 'maintenance', 'work', 'delay', 'indigenous', 'manufacturing', 'plan', 'parallel', 'platform', 'necessitated', 'new', 'defence', 'deal', 'submarine', 'expected', 'buy', 'scorpene', 'submarine', 'group', 'government', 'source', 'said', 'price', 'deal', 'agreed', 'expected', 'agree', 'buy', 'jet', 'source', 'said', 'giving', 'deal', 'expected', 'value', 'marine', 'version', 'jet', 'intended', 'indigenous', 'aircraft', 'carrier', 'commissioned', 'outperformed', 'test', 'year', 'indian', 'requirement', 'visit', 'host', 'private', 'dinner', 'state', 'banquet', 'meet', 'political', 'leader', 'selected', 'french', 'personality', 'business', 'leader', 'interact', 'indian', 'diaspora', 'island', 'territory', 'deep', 'interest', 'concerned', 'growing', 'assertiveness', 'region', 'detail', 'announcement', 'plan', 'region', 'known', 'visit', 'comes', 'month', 'hosted', 'state', 'visit', 'offered', 'critical', 'military', 'technology', 'including', 'fighter', 'jet', 'engine', 'high', 'altitude', 'drone', 'week', 'chaired', 'online', 'meeting', 'leader', 'member', 'included', 'russian', 'leader', 'member', 'country', 'expected', 'visit', 'summit', 'hosted', 'head', 'year', 'indian', 'leader', 'later', 'visit', 'meet', 'ruler']\n",
      "Number of unique words: 239\n",
      "indian: 6\n",
      "jet: 6\n",
      "deal: 5\n",
      "year: 5\n",
      "leader: 5\n",
      "french: 4\n",
      "plan: 4\n",
      "visit: 4\n",
      "strategic: 3\n",
      "defence: 3\n",
      "['indian', 'jet', 'deal', 'year', 'leader', 'french', 'plan', 'visit', 'strategic', 'defence']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/asia/more-submarines-jets-indian-navy-cards-modi-visits-france-3623756\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53bc983",
   "metadata": {},
   "source": [
    "## Commentary: Cut plastic packaging? Not so straightforward in modern Singapore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "382140ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      Commentary\n",
      "  \n",
      "Plastic use in supermarkets extends far beyond disposable carrier bags at checkout counters. But there are few easy, plastic-free alternatives for storing and transporting perishable food, says cookbook author Pamelia Chia.BREDA, The Netherlands: The mandatory plastic bag charge of at least S$0.05 (US$0.04) kicked in at large supermarkets across Singapore on Monday (Jul 3). First announced in March 2022, the charge aims to nudge consumers to be mindful of and reduce their usage of disposable bags, and is a step towards tackling the ecological bane of single-use plastic.Only 6 per cent of plastic waste was recycled in Singapore in 2021 and 2022. The rest of the waste ends up in incinerators, and the ash residues are then buried in Semakau Landfill, which is estimated to run out of space by 2035.Plastic in supermarkets extends far beyond the disposable bags at checkout counters. On a recent trip home to Singapore, I was appalled to see nearly every piece of vegetable and fruit shrink-wrapped in plastic.My initial reaction was outrage: Is it truly necessary and sensible to wrap something as fleeting as food in plastic, a non-biodegradable material that lasts forever?But as I pondered alternatives, I realised that there are no easy answers.Plastic packaging serves the critical function of slowing decay and extending the shelf life of perishable food items. Strawberries, for example, when nestled into clamshell packaging and stacked in light, sturdy crates, are protected during transport - there is minimal bruising and damage, if at all.Leafy vegetables, when wrapped in impermeable plastic sleeves, also stay green longer. This reduces food waste all along the supply chain - in the world of agriculture and grocery retail, where every less-than-perfect fruit or vegetable represents a loss of profit, the extended shelf life makes a big difference.When I lived in Australia, I enjoyed shopping at Terra Madre, Melbourne’s largest organic wholesaler which aims to minimise its impact on the environment across all aspects of its business. It does this through establishing relationships with their farmers and suppliers, who ensure that much of its produce is delivered in reusable crates. The staff at Terra Madre also sort the polystyrene and plastic packaging used and send it back to the farmers.In a country like Singapore, however, where more than 90 per cent of its food is imported rather than locally produced, this model can be hard to achieve.Doing away with plastic packaging would also require consumers to alter their shopping and eating patterns. My grandmother belonged to the generation that would make a trip to the wet market every day to procure the freshest produce, buying only what her push trolley would fit.While it is a wonderful practice, it can be impractical for many Singapore residents who lead busy, modern lives. With cars and refrigerators being common, many consumers have shifted away to shopping for groceries for an entire week, buying what they can load into their car boots and backseats.This consumption pattern means that shoppers look for products that last longer. Enter plastic packaging.What might help alleviate our dependency on plastic packaging is widespread education about produce storage. During my time working in restaurants, the entire team would often stop what they were doing when suppliers arrived to unpack produce and store them in a way that keeps them fresh for longer.This meant wrapping bunches of herbs in dampened kitchen towels and stowing them in airtight storage containers, for example. Good storage habits such as these are just as relevant in the space of a home, keeping produce fresh for longer while simultaneously reducing our reliance on plastic packaging. As supermarkets worldwide grapple with their culpability in climate change, a variety of solutions have emerged. Thai, Vietnamese and Filipino supermarkets have begun using banana leaves to wrap vegetables, a practice that is not uncommon at traditional markets in these countries.While creative, the banana leaves only serve to bundle up produce - they do not provide the same airtight protection for meat, seafood, vegetables, and fruit as plastic does. Also, banana leaves as packaging might pose problems for those suffering from latex allergies.Zero waste stores are abundant in the Netherlands, where I live. Large dispensers house everything from nuts to rice, and consumers are encouraged to bring their own bags, jars, or containers. While bulk has a buy-only-what-you-need appeal, it requires a whole overhauling of the grab-and-go supermarket experience.Cross-contamination also poses another challenge. UnPackt, the first zero waste store to open in Singapore, has reported customers not respecting the hygiene required to keep the packaging-free foods fresh. Because of this, to supermarkets, bulk remains a liability that outstrips its environmental virtues.It is clear that there isn’t a magic bullet that eradicates plastic in supermarkets. A carrot-and-stick approach, where Singaporeans are rewarded for positive climate action or given disincentives as negative reinforcement, however, does not truly reach the heart of the matter.In some cases, it might even backfire. Though Singapore supermarkets said that proceeds from the plastic bag charge would be channelled towards social or environmental causes, slapping a fee on single-use plastics can be perceived by some as thinly veiled capitalism. Care for the environment has to come from within. I have observed that in the Netherlands and Australia, it is mainstream for both young and old to bring their own tote bags to the supermarket, proving that it is not simply the habit of a few climate activists.Beyond plastic consumption, there are also other major lifestyle factors with devastating environmental impacts when compounded in the long term. These are behaviours that we can easily change, such as driving in a small country with world-class transport, reliance on food delivery services and an overconsumption of meat.For truly sustainable change to happen, Singapore has to make the shift to a culture that prioritises the environment over convenience. But until then, 5 cents a bag is a start.Pamelia Chia is the author of the cookbooks Wet Market to Table and Plantasia: A Vegetarian Cookbook Through Asia, and the founder of Singapore Noodles, a newsletter with the mission of keeping Singapore's food heritage alive.\n",
      "['use', 'supermarket', 'extends', 'far', 'disposable', 'carrier', 'bag', 'checkout', 'counter', 'easy', 'plastic', 'free', 'alternative', 'storing', 'transporting', 'perishable', 'food', 'says', 'cookbook', 'author', 'mandatory', 'plastic', 'bag', 'charge', 'kicked', 'large', 'supermarket', 'announced', 'charge', 'aims', 'nudge', 'consumer', 'mindful', 'reduce', 'usage', 'disposable', 'bag', 'step', 'tackling', 'ecological', 'bane', 'single', 'use', 'plastic', 'cent', 'plastic', 'waste', 'recycled', 'rest', 'waste', 'ends', 'incinerator', 'ash', 'residue', 'buried', 'estimated', 'run', 'space', '2035.plastic', 'supermarket', 'extends', 'far', 'disposable', 'bag', 'checkout', 'counter', 'recent', 'trip', 'home', 'appalled', 'nearly', 'piece', 'vegetable', 'fruit', 'shrink', 'wrapped', 'plastic', 'initial', 'reaction', 'outrage', 'truly', 'necessary', 'sensible', 'wrap', 'fleeting', 'food', 'plastic', 'non', 'biodegradable', 'material', 'lasts', 'pondered', 'alternative', 'realised', 'easy', 'answer', 'plastic', 'packaging', 'serves', 'critical', 'function', 'slowing', 'decay', 'extending', 'shelf', 'life', 'perishable', 'food', 'item', 'strawberry', 'example', 'nestled', 'clamshell', 'packaging', 'stacked', 'light', 'sturdy', 'crate', 'protected', 'transport', 'minimal', 'bruising', 'damage', 'leafy', 'vegetable', 'wrapped', 'impermeable', 'plastic', 'sleeve', 'stay', 'green', 'long', 'reduces', 'food', 'waste', 'supply', 'chain', 'world', 'agriculture', 'grocery', 'retail', 'perfect', 'fruit', 'vegetable', 'represents', 'loss', 'profit', 'extended', 'shelf', 'life', 'makes', 'big', 'difference', 'lived', 'enjoyed', 'shopping', 'large', 'organic', 'wholesaler', 'aims', 'minimise', 'impact', 'environment', 'aspect', 'business', 'establishing', 'relationship', 'farmer', 'supplier', 'ensure', 'produce', 'delivered', 'reusable', 'crate', 'staff', 'sort', 'polystyrene', 'plastic', 'packaging', 'send', 'farmer', 'country', 'cent', 'food', 'imported', 'locally', 'produced', 'model', 'hard', 'achieve', 'away', 'plastic', 'packaging', 'require', 'consumer', 'alter', 'shopping', 'eating', 'pattern', 'grandmother', 'belonged', 'generation', 'trip', 'wet', 'market', 'day', 'procure', 'fresh', 'produce', 'buying', 'push', 'trolley', 'fit', 'wonderful', 'practice', 'impractical', 'resident', 'lead', 'busy', 'modern', 'life', 'car', 'refrigerator', 'common', 'consumer', 'shifted', 'away', 'shopping', 'grocery', 'entire', 'week', 'buying', 'load', 'car', 'boot', 'backseat', 'consumption', 'pattern', 'means', 'shopper', 'look', 'product', 'long', 'Enter', 'plastic', 'packaging', 'help', 'alleviate', 'dependency', 'plastic', 'packaging', 'widespread', 'education', 'produce', 'storage', 'time', 'working', 'restaurant', 'entire', 'team', 'stop', 'supplier', 'arrived', 'unpack', 'produce', 'store', 'way', 'keeps', 'fresh', 'long', 'meant', 'wrapping', 'bunche', 'herb', 'dampened', 'kitchen', 'towel', 'stowing', 'airtight', 'storage', 'container', 'example', 'good', 'storage', 'habit', 'relevant', 'space', 'home', 'keeping', 'produce', 'fresh', 'long', 'simultaneously', 'reducing', 'reliance', 'plastic', 'packaging', 'supermarket', 'worldwide', 'grapple', 'culpability', 'climate', 'change', 'variety', 'solution', 'emerged', 'filipino', 'supermarket', 'begun', 'banana', 'leave', 'wrap', 'vegetable', 'practice', 'uncommon', 'traditional', 'market', 'country', 'creative', 'banana', 'leaves', 'serve', 'bundle', 'produce', 'provide', 'airtight', 'protection', 'meat', 'seafood', 'vegetable', 'fruit', 'plastic', 'banana', 'leave', 'packaging', 'pose', 'problem', 'suffering', 'latex', 'allergy', 'waste', 'store', 'abundant', 'live', 'large', 'dispenser', 'house', 'nut', 'rice', 'consumer', 'encouraged', 'bring', 'bag', 'jar', 'container', 'bulk', 'buy', 'need', 'appeal', 'requires', 'overhauling', 'grab', 'supermarket', 'experience', 'cross', 'contamination', 'poses', 'challenge', 'waste', 'store', 'open', 'reported', 'customer', 'respecting', 'hygiene', 'required', 'packaging', 'free', 'food', 'fresh', 'supermarket', 'bulk', 'remains', 'liability', 'outstrips', 'environmental', 'virtue', 'clear', 'magic', 'bullet', 'eradicates', 'plastic', 'supermarket', 'carrot', 'stick', 'approach', 'rewarded', 'positive', 'climate', 'action', 'given', 'disincentive', 'negative', 'reinforcement', 'truly', 'reach', 'heart', 'matter', 'case', 'backfire', 'supermarket', 'said', 'proceeds', 'plastic', 'bag', 'charge', 'channelled', 'social', 'environmental', 'cause', 'slapping', 'fee', 'single', 'use', 'plastic', 'perceived', 'thinly', 'veiled', 'capitalism', 'environment', 'come', 'observed', 'mainstream', 'young', 'old', 'bring', 'tote', 'bag', 'supermarket', 'proving', 'simply', 'habit', 'climate', 'activist', 'plastic', 'consumption', 'major', 'lifestyle', 'factor', 'devastating', 'environmental', 'impact', 'compounded', 'long', 'term', 'behaviour', 'easily', 'change', 'driving', 'small', 'country', 'world', 'class', 'transport', 'reliance', 'food', 'delivery', 'service', 'overconsumption', 'meat', 'truly', 'sustainable', 'change', 'happen', 'shift', 'culture', 'prioritises', 'environment', 'convenience', 'cent', 'bag', 'start', 'author', 'cookbook', 'table', 'founder', 'newsletter', 'mission', 'keeping', 'food', 'heritage', 'alive']\n",
      "Number of unique words: 492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plastic: 18\n",
      "supermarket: 10\n",
      "packaging: 9\n",
      "bag: 8\n",
      "food: 8\n",
      "waste: 5\n",
      "vegetable: 5\n",
      "long: 5\n",
      "consumer: 4\n",
      "produce: 4\n",
      "['plastic', 'supermarket', 'packaging', 'bag', 'food', 'waste', 'vegetable', 'long', 'consumer', 'produce']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/commentary/supermarket-plastic-bag-ban-singapore-packaging-waste-sustainability-3606836\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cab9f7",
   "metadata": {},
   "source": [
    "## E. coli in chicken rice? Stalls are subject to more stringent checks, says SFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4426fa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      Singapore\n",
      "  \n",
      "A recent YouTube video claimed that nearly half of 100 chicken rice samples had more E. coli bacteria than allowed under Singapore's regulations.Whole chickens on display at room temperature. (File photo: AFP/Roslan Rahman)SINGAPORE: Chicken rice stalls are subject to more stringent and frequent checks as the dish poses higher food safety risks, the Singapore Food Agency (SFA) said on Friday (Jun 9). The agency was responding to CNA's queries about a YouTube video saying that nearly half of 100 chicken rice samples tested in a lab were found to have exceeded Singapore's regulatory limits for E. coli bacteria.A common preparation method for the dish involves boiling the chicken before placing it in cold water, which means the meat may be undercooked.The chickens are also often displayed at room temperature for hours, where bacteria that survived the cooking process may continue to grow.\"When chickens are sliced to serve, cross-contamination could occur from handling meat or through the chopping boards and other surfaces,\" SFA said.SFA said it looks into public feedback and monitors food stalls by collecting samples and testing them for microbiological contamination to ensure food safety requirements are met. The agency received 126 instances of feedback on chicken rice stalls from 2020 to May this year. It took enforcement action against one stall for selling unsafe food. \"In addition, enforcement actions were taken against 22 other chicken rice stalls found to fail our standards under SFA's Market Monitoring Programme,\" the agency said, adding that the rate of failure due to E. coli detection in chicken rice samples has decreased in recent years.Nearly 40 per cent of chicken rice samples tested in 2021 exceeded the regulatory limit for E. coli bacteria in ready-to-eat food, according to SFA's website. In 2022, that figure was 13 per cent. Pathogens were only found in 0.9 per cent of chicken rice samples in both years.\"There has been no reported foodborne illness cases involving chicken rice since 2020,\" SFA said.Most strains of E. coli do not cause illnesses. It is commonly used as an indicator of safety because the presence of high levels of the bacteria is linked to poor hygiene, cross-contamination, poor storage practices and insufficient cooking.SFA said it will continue to remind chicken rice stallholders about food safety practices.YouTuber Angel Hsu posted a video on Jun 3 about visiting 100 chicken rice stalls to find the best one. But she also bought an additional packet that was sealed in a food-grade bag and put in an ice box before being sent to a lab for testing.All components of the chicken rice packet including the chilli, cucumber and garnish were tested in the laboratory, Ms Hsu said in response to queries.According to data she shared online, 45 stalls had E. coli bacteria exceeding SFA's limit of 100 colony-forming units per gram (cfu/g). Four stalls had a reading of more than 490,000 cfu/g, which is the upper limit of the lab's machine.Ms Hsu, who used to work in a microbiology lab, said she had diarrhoea and stomachaches every day while filming the video but did not take any tests to determine if she had a bacterial infection.She told CNA she made the video to show tourists that there are many good chicken rice stalls in Singapore beyond the famous ones. \"Also, my university professor used to mention that steamed chicken is the best medium for bacteria growth hence this video idea came about,\" she said.Medical doctor Leslie Tay, who also runs popular food blog ieatishootipost, said he commends Ms Hsu's effort but pointed out that some places may have a lower bacterial count because they were sampled right after the chicken was cooked.\"The longer the chicken hangs at room temperature, the higher the risk,\" he wrote on Facebook, advising readers to eat chicken rice for lunch instead of dinner.But he added that high E. coli counts do not always lead to outbreaks because only a few strains cause serious illness.Angel Hsu ate the chicken rice from 100 stalls in Singapore to find the best one but at the same time she also found...SFA said food operators are advised to sell cooked chicken meat within four hours of preparation and replace the water bath frequently. There should also be proper segregation between cooked and raw meat.Food handlers should wash their hands thoroughly after touching raw food and use gloves when handling ready-to-eat food. Cutting boards, cleaning cloths and surfaces should be cleaned regularly.Dr Tay said the risk of cross-infection from raw to cooked chicken can be very high since hawker stalls are small. That said, he noted that Ms Hsu did not fall seriously sick during her experiment.\"After two months of eating chicken rice several times a day, she hasn't actually gotten salmonellosis or severe illness. It's actually quite a good result!\"Despite potential risks related to soaking cooked chicken in cold water after cooking, Dr Tay told CNA it could be difficult to change that method of preparation. \"Unless Singaporeans are willing to sacrifice our chicken rice and eat very tough chicken that has no 'jelly' and all these things. That is the consequence of putting in too many rules, then you have to give up eating nice chicken rice and we'll lose a national icon.\"Food safety is of paramount importance, but there needs to be a good balance,\" he said.\n",
      "['recent', 'video', 'claimed', 'nearly', 'half', 'chicken', 'rice', 'sample', 'coli', 'bacteria', 'allowed', 'regulation', 'chicken', 'display', 'room', 'temperature', 'file', 'photo', 'Rahman)SINGAPORE', 'rice', 'stall', 'subject', 'stringent', 'frequent', 'check', 'dish', 'poses', 'high', 'food', 'safety', 'risk', 'said', 'agency', 'responding', 'query', 'video', 'saying', 'nearly', 'half', 'chicken', 'rice', 'sample', 'tested', 'lab', 'found', 'exceeded', 'regulatory', 'limit', 'coli', 'bacteria', 'common', 'preparation', 'method', 'dish', 'involves', 'boiling', 'chicken', 'placing', 'cold', 'water', 'means', 'meat', 'undercooked', 'chicken', 'displayed', 'room', 'temperature', 'hour', 'bacteria', 'survived', 'cooking', 'process', 'continue', 'grow', 'chicken', 'sliced', 'serve', 'cross', 'contamination', 'occur', 'handling', 'meat', 'chopping', 'board', 'surface', 'said', 'said', 'looks', 'public', 'feedback', 'monitors', 'food', 'stall', 'collecting', 'sample', 'testing', 'microbiological', 'contamination', 'ensure', 'food', 'safety', 'requirement', 'met', 'agency', 'received', 'instance', 'feedback', 'chicken', 'rice', 'stall', 'year', 'took', 'enforcement', 'action', 'stall', 'selling', 'unsafe', 'food', 'addition', 'enforcement', 'action', 'taken', 'chicken', 'rice', 'stall', 'found', 'fail', 'standard', 'agency', 'said', 'adding', 'rate', 'failure', 'coli', 'detection', 'chicken', 'rice', 'sample', 'decreased', 'recent', 'year', 'nearly', 'cent', 'chicken', 'rice', 'sample', 'tested', 'exceeded', 'regulatory', 'limit', 'coli', 'bacteria', 'ready', 'eat', 'food', 'according', 'website', 'figure', 'cent', 'found', 'cent', 'chicken', 'rice', 'sample', 'year', '\"there', 'reported', 'foodborne', 'illness', 'case', 'involving', 'chicken', 'rice', 'said', 'strain', 'coli', 'cause', 'illness', 'commonly', 'indicator', 'safety', 'presence', 'high', 'level', 'bacteria', 'linked', 'poor', 'hygiene', 'cross', 'contamination', 'poor', 'storage', 'practice', 'insufficient', 'cooking', 'said', 'continue', 'remind', 'chicken', 'rice', 'stallholder', 'food', 'safety', 'practice', 'posted', 'video', 'visiting', 'chicken', 'rice', 'stall', 'find', 'good', 'bought', 'additional', 'packet', 'sealed', 'food', 'grade', 'bag', 'ice', 'box', 'sent', 'lab', 'testing', 'component', 'chicken', 'rice', 'packet', 'including', 'garnish', 'tested', 'laboratory', 'said', 'response', 'query', 'According', 'datum', 'shared', 'online', 'stall', 'coli', 'bacteria', 'exceeding', 'limit', 'colony', 'forming', 'unit', 'gram', 'stall', 'reading', 'cfu', 'upper', 'limit', 'lab', 'machine', 'work', 'microbiology', 'lab', 'said', 'diarrhoea', 'stomachache', 'day', 'filming', 'video', 'test', 'determine', 'bacterial', 'infection', 'told', 'video', 'tourist', 'good', 'chicken', 'rice', 'stall', 'famous', 'one', 'university', 'professor', 'mention', 'steamed', 'good', 'medium', 'bacteria', 'growth', 'video', 'idea', 'came', 'said', 'medical', 'doctor', 'runs', 'popular', 'food', 'blog', 'ieatishootipost', 'said', 'commends', 'effort', 'pointed', 'place', 'low', 'bacterial', 'count', 'sampled', 'right', 'chicken', 'cooked', '\"The', 'long', 'chicken', 'hangs', 'room', 'temperature', 'high', 'risk', 'wrote', 'advising', 'reader', 'eat', 'chicken', 'rice', 'lunch', 'instead', 'dinner', 'added', 'high', 'coli', 'count', 'lead', 'outbreak', 'strain', 'cause', 'illness', 'ate', 'chicken', 'rice', 'stall', 'find', 'good', 'time', 'found', 'said', 'food', 'operator', 'advised', 'sell', 'cooked', 'chicken', 'meat', 'hour', 'preparation', 'replace', 'water', 'bath', 'frequently', 'proper', 'segregation', 'cooked', 'raw', 'meat', 'food', 'handler', 'wash', 'hand', 'thoroughly', 'touching', 'raw', 'food', 'use', 'glove', 'handling', 'ready', 'eat', 'food', 'Cutting', 'board', 'cleaning', 'cloth', 'surface', 'cleaned', 'regularly', 'said', 'risk', 'cross', 'infection', 'raw', 'cooked', 'chicken', 'high', 'hawker', 'stall', 'small', 'said', 'noted', 'fall', 'seriously', 'sick', 'experiment', '\"after', 'month', 'eating', 'chicken', 'rice', 'time', 'day', 'actually', 'gotten', 'salmonellosis', 'severe', 'illness', 'actually', 'good', 'result!\"despite', 'potential', 'risk', 'related', 'soaking', 'cooked', 'chicken', 'cold', 'water', 'cooking', 'told', 'difficult', 'change', 'method', 'preparation', 'willing', 'sacrifice', 'chicken', 'rice', 'eat', 'tough', 'chicken', 'jelly', 'thing', 'consequence', 'putting', 'rule', 'eating', 'nice', 'chicken', 'rice', 'lose', 'national', 'icon', 'safety', 'paramount', 'importance', 'needs', 'good', 'balance', 'said']\n",
      "Number of unique words: 454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chicken: 27\n",
      "rice: 18\n",
      "food: 12\n",
      "stall: 11\n",
      "coli: 7\n",
      "bacteria: 7\n",
      "video: 6\n",
      "sample: 6\n",
      "good: 6\n",
      "high: 5\n",
      "['chicken', 'rice', 'food', 'stall', 'coli', 'bacteria', 'video', 'sample', 'good', 'high']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/singapore/chicken-rice-e-coli-youtube-video-stringent-checks-sfa-3550506\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc231c",
   "metadata": {},
   "source": [
    "## Unsold greens a persistent issue for Singapore's vegetable farms; businesses urge support for local produce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "aa680f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      Singapore\n",
      "  \n",
      "Reasons for the wastage include order cancellations, poor product displays at retailers, low awareness and a lack of support for the local farming scene, the farms said.Employees harvest greens at an urban vegetable farm in Singapore.SINGAPORE: Some urban farms in Singapore are throwing away extra vegetables, after seeing a surge in leftover greens as supply overwhelms demand.Reasons for the wastage include order cancellations from clients, poor product displays at retailers, and low awareness of the local farming scene, they said.The businesses added they are hoping for more initiatives to boost recognition and support for locally farmed produce.SG Veg Farms, which harvests up to 500 kg of hydroponically grown greens daily, has seen its volume of leftovers increase by about 40 per cent over the past few months.This was largely due to cheaper imports from regional countries luring consumers away, said the farm’s director Eyleen Goh.The business usually donates leftover vegetables to nursing homes or needy families but as a last resort, some greens have to be thrown away.“We spend a lot of time and effort growing these plants and we don’t want our vegetables to go to waste,” said Ms Goh.“But sometimes it’s more difficult for us to donate because we need to arrange logistics, etc., and it is easier for us to throw them away but it is such a waste.”At ComCrop, a rooftop urban farm along Woodlands Loop, about 30 per cent of its produce is donated to charities.Still, the volume of unsold vegetables is an improvement from last year, when about half its harvest had to be given away.“The leftovers are decreasing, but it's still a problem. We still have wastage that we are having to donate on a daily basis,” said the company’s chief executive officer Peter Barber.The farm said poor product placement at stores is among the challenges faced.“The biggest feedback we have had from Singaporeans is that they can't find our produce,” said Mr Barber. “So, we have been working closely with (supermarkets) to get better shelf positioning.”Both farms said low consumer awareness and a lack of support for local produce are major issues. SG Veg Farms, which is located atop a multi-storey carpark along Admiralty Drive, said that even residents in the neighbourhood are unaware of its presence.“Many residents told us they are not aware there’s a commercial farm here. This is the kind of difficulty that we are facing – trying to reach out to even those nearby,” Ms Goh said.“Authorities told us we should reach out on our own to the consumers and the public on awareness, and also educate people about supporting local produce. But we are just a farm, it is very difficult for us to do everything on our own.”The price of local produce can also deter consumers, as they tend to cost about 30 per cent more compared to imported vegetables as a result of limited space for farming, and high labour and energy costs.However, ComCrop’s Mr Barber urged shoppers to understand that locally-grown crops from certified farms can be of higher standards due to quality assurance schemes.“We're clean and green certified. This is a really hard qualification to achieve and so (we are) of a much higher standard than a lot of the stuff that are coming in from other countries,” he said, referring to the Singapore Clean and Green Urban Farms initiative that awards farms that adopt sustainable practices.“When buying local produce, people need to look at the quality as well as the price, then they will realise that actually, we are really value for money,” he added.The farms have initiated activities and educational sessions to create awareness for their facilities and vegetables, as well as counted on word-of-mouth marketing from regular customers.However, they hope authorities can do more to promote local greens by emphasising the importance of food security and self-sustainability.“Singaporeans are not really used to having local farms and are very used to getting imported foods,” said Ms Goh.“But food is so important. It's just like security. I mean, there's no point having a very, very strong army if we don’t already have our own food, right?“Our farm is trying to help our country reach the 30 by 30 goal to produce our own food and make sure that we don't rely on external sources,” she added, referring to the Singapore Food Agency’s (SFA) plan to locally produce 30 per cent of the nation’s nutritional needs by 2030.Ms Goh also lauded initiatives such as the SFA’s Farm-to-Table Recognition Programme, which acknowledges hotels, restaurants and caterers for using at least 15 per cent of local produce in their menus by awarding them a logo to display.However, she said consumers do not seem motivated by the labels to throw their support behind those businesses.“Businesses have told me, ‘why would I want that sticker, since consumers don’t really care?’ The incentive to use local produce is actually not that strong,” she noted.“Still, it is a good programme – at least it is encouraging people and organisations to use more local produce. I hope these kinds of initiatives can be enhanced (to entice more people to support local greens).”\n",
      "['wastage', 'include', 'order', 'cancellation', 'poor', 'product', 'display', 'retailer', 'low', 'awareness', 'lack', 'support', 'local', 'farming', 'scene', 'farm', 'said', 'employee', 'harvest', 'green', 'urban', 'vegetable', 'farm', 'urban', 'farm', 'throwing', 'extra', 'vegetable', 'seeing', 'surge', 'green', 'supply', 'overwhelm', 'demand', 'reason', 'wastage', 'include', 'order', 'cancellation', 'client', 'poor', 'product', 'display', 'retailer', 'low', 'awareness', 'local', 'farming', 'scene', 'said', 'business', 'added', 'hoping', 'initiative', 'boost', 'recognition', 'support', 'locally', 'farmed', 'produce', 'harvests', 'kg', 'hydroponically', 'grown', 'green', 'daily', 'seen', 'volume', 'leftover', 'increase', 'cent', 'past', 'month', 'largely', 'cheap', 'import', 'regional', 'country', 'luring', 'consumer', 'away', 'said', 'farm', 'director', 'business', 'usually', 'donates', 'leftover', 'vegetable', 'nursing', 'home', 'needy', 'family', 'resort', 'green', 'thrown', 'away', '“we', 'spend', 'lot', 'time', 'effort', 'growing', 'plant', 'want', 'vegetable', 'waste', 'said', 'difficult', 'donate', 'need', 'arrange', 'logistic', 'easy', 'throw', 'away', 'waste', 'rooftop', 'urban', 'farm', 'cent', 'produce', 'donated', 'charity', 'volume', 'unsold', 'vegetable', 'improvement', 'year', 'harvest', 'given', 'leftover', 'decreasing', 'problem', 'wastage', 'having', 'donate', 'daily', 'basis', 'said', 'company', 'chief', 'executive', 'officer', 'farm', 'said', 'poor', 'product', 'placement', 'store', 'challenge', 'faced', '“The', 'big', 'feedback', 'find', 'produce', 'said', 'working', 'closely', 'supermarket', 'well', 'shelf', 'positioning', '”both', 'farm', 'said', 'low', 'consumer', 'awareness', 'lack', 'support', 'local', 'produce', 'major', 'issue', 'located', 'multi', 'storey', 'carpark', 'said', 'resident', 'neighbourhood', 'unaware', 'presence', '“many', 'resident', 'told', 'aware', 'commercial', 'farm', 'kind', 'difficulty', 'facing', 'trying', 'reach', 'nearby', 'said', '“authoritie', 'told', 'reach', 'consumer', 'public', 'awareness', 'educate', 'people', 'supporting', 'local', 'produce', 'farm', 'difficult', 'price', 'local', 'produce', 'deter', 'consumer', 'tend', 'cost', 'cent', 'compared', 'imported', 'vegetable', 'result', 'limited', 'space', 'farming', 'high', 'labour', 'energy', 'cost', 'urged', 'shopper', 'understand', 'locally', 'grown', 'crop', 'certified', 'farm', 'high', 'standard', 'quality', 'assurance', 'scheme', \"“we're\", 'clean', 'green', 'certified', 'hard', 'qualification', 'achieve', 'high', 'standard', 'lot', 'stuff', 'coming', 'country', 'said', 'referring', 'initiative', 'award', 'farm', 'adopt', 'sustainable', 'practice', 'buying', 'local', 'produce', 'people', 'need', 'look', 'quality', 'price', 'realise', 'actually', 'value', 'money', 'added', 'farm', 'initiated', 'activity', 'educational', 'session', 'create', 'awareness', 'facility', 'vegetable', 'counted', 'word', 'mouth', 'marketing', 'regular', 'customer', 'hope', 'authority', 'promote', 'local', 'green', 'emphasising', 'importance', 'food', 'security', 'self', 'sustainability', '“singaporean', 'having', 'local', 'farm', 'getting', 'imported', 'food', 'said', '“but', 'food', 'important', 'security', 'mean', 'point', 'having', 'strong', 'army', 'food', 'farm', 'trying', 'help', 'country', 'reach', 'goal', 'produce', 'food', 'sure', 'rely', 'external', 'source', 'added', 'referring', 'plan', 'locally', 'produce', 'cent', 'nation', 'nutritional', 'need', 'lauded', 'initiative', 'farm', 'acknowledges', 'hotel', 'restaurant', 'caterer', 'cent', 'local', 'produce', 'menu', 'awarding', 'logo', 'display', 'said', 'consumer', 'motivated', 'label', 'throw', 'support', 'business', '“businesse', 'told', 'want', 'sticker', 'consumer', 'care', 'incentive', 'use', 'local', 'produce', 'actually', 'strong', 'noted', '“still', 'good', 'programme', 'encouraging', 'people', 'organisation', 'use', 'local', 'produce', 'hope', 'kind', 'initiative', 'enhanced', 'entice', 'people', 'support', 'local', 'green']\n",
      "Number of unique words: 394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "farm: 15\n",
      "local: 12\n",
      "produce: 10\n",
      "green: 7\n",
      "vegetable: 7\n",
      "consumer: 6\n",
      "awareness: 5\n",
      "cent: 5\n",
      "food: 5\n",
      "support: 4\n",
      "['farm', 'local', 'produce', 'green', 'vegetable', 'consumer', 'awareness', 'cent', 'food', 'support']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/singapore/urban-rooftop-farming-vegetables-greens-unsold-leftover-lack-support-3623701\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f705d37",
   "metadata": {},
   "source": [
    "## With 735 million people hungry, UN says world is 'off track' to meet its 2030 goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "232fcf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      World\n",
      "  \n",
      "Residents pick up free groceries at a food pantry run by La Colaborativa, as the U.S. is cutting benefits delivered through the Supplemental Nutrition Assistance Program (SNAP) by the end of March which kept millions from going hungry through the COVID-19 pandemic, in Chelsea, Massachusetts, U.S., March 8, 2023.     REUTERS/Brian SnyderAbout 735 million people worldwide faced chronic hunger in 2022, a figure much higher than before the COVID-19 pandemic and which threatens progress towards a global goal to end hunger by 2030, said the United Nations on Wednesday (Jul 12).A multi-year upward trend in hunger rates levelled off last year as many countries recovered economically from the pandemic, but the war in Ukraine and its pressure on food and energy prices offset some of those gains, the UN said in its annual State of Food Security and Nutrition in the World (SOFI) report.The result is that an estimated 122 million more people were hungry in 2022 than in 2019 and the world is \"far off track\" to meet the UN's Sustainable Development Goal of ending hunger by 2030, said the report. Instead, the report projects that 600 million people will be undernourished in 2030.\"We are seeing that hunger is stabilizing at a high level, which is bad news,\" said Maximo Torero Cullen, chief economist of the UN's Food and Agriculture Organization (FAO), in an interview with Reuters.The main drivers of global hunger in recent years were conflict-driven disruption to livelihoods, climate extremes that threatened agricultural production, and economic hardship exacerbated by the pandemic, the report said.Some parts of the world have seen hunger decline, including South America and most regions in Asia. But in the Caribbean, Western Asia, and Africa, hunger is rising.To change the trend, nations must pair humanitarian aid with strengthening local food supply chains, said Kevin Mugenya, the food systems director for Mercy Corps, an international aid group, in an interview with Reuters.\"Countries need to have localized solutions,\" he said.The report was compiled by the UN's International Fund for Agricultural Development, Children's Fund, World Health Organization, World Food Programme, and FAO.\n",
      "['resident', 'pick', 'free', 'grocery', 'food', 'pantry', 'run', 'cutting', 'benefit', 'delivered', 'end', 'kept', 'million', 'going', 'hungry', 'pandemic', 'people', 'worldwide', 'faced', 'chronic', 'hunger', 'figure', 'high', 'covid-19', 'pandemic', 'threatens', 'progress', 'global', 'goal', 'end', 'hunger', 'said', '12).a', 'multi', 'year', 'upward', 'trend', 'hunger', 'rate', 'levelled', 'year', 'country', 'recovered', 'economically', 'pandemic', 'war', 'pressure', 'food', 'energy', 'price', 'offset', 'gain', 'said', 'annual', 'report', 'result', 'estimated', 'people', 'hungry', 'world', 'far', 'track', 'meet', 'sustainable', 'ending', 'hunger', 'said', 'report', 'instead', 'report', 'projects', 'people', 'undernourished', 'seeing', 'hunger', 'stabilizing', 'high', 'level', 'bad', 'news', 'said', 'chief', 'economist', 'interview', 'main', 'driver', 'global', 'hunger', 'recent', 'year', 'conflict', 'driven', 'disruption', 'livelihood', 'climate', 'extreme', 'threatened', 'agricultural', 'production', 'economic', 'hardship', 'exacerbated', 'pandemic', 'report', 'said', 'part', 'world', 'seen', 'hunger', 'decline', 'including', 'region', 'hunger', 'rising', 'change', 'trend', 'nation', 'pair', 'humanitarian', 'aid', 'strengthening', 'local', 'food', 'supply', 'chain', 'said', 'food', 'system', 'director', 'international', 'aid', 'group', 'interview', '\"countrie', 'need', 'localized', 'solution', 'said', 'report', 'compiled']\n",
      "Number of unique words: 140\n",
      "hunger: 8\n",
      "report: 5\n",
      "food: 4\n",
      "pandemic: 4\n",
      "people: 3\n",
      "year: 3\n",
      "hungry: 2\n",
      "high: 2\n",
      "global: 2\n",
      "trend: 2\n",
      "['hunger', 'report', 'food', 'pandemic', 'people', 'year', 'hungry', 'high', 'global', 'trend']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/world/united-nations-world-track-meet-2030-goal-3624306\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c3ed9",
   "metadata": {},
   "source": [
    "## Transport Minister S Iswaran assisting in CPIB investigation, instructed to take leave of absence by PM Lee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d20a691c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      Singapore\n",
      "  \n",
      "The case was uncovered by the anti-graft agency and Mr Iswaran has been instructed to take a leave of absence until the investigation is completed. SINGAPORE: Transport Minister S Iswaran is assisting the Corrupt Practices Investigation Bureau (CPIB) with an investigation into a case uncovered by the agency. The bureau did not elaborate on the nature of the investigation in its statement on Wednesday (Jul 12). Prime Minister Lee Hsien Loong said in a separate statement that the investigation would require Mr Iswaran, as well as other individuals, to be interviewed by CPIB.Mr Lee said he was briefed by the director of CPIB last Wednesday regarding a case that the bureau had uncovered. The director sought Mr Lee’s concurrence to open a formal investigation.Mr Denis Tang is the director of the anti-graft agency, which sits under the Prime Minister's Office.\"I gave Director CPIB my concurrence on Jul 6, following which the formal investigation began on Jul 11,\" said the Prime Minister. Mr Lee said he has instructed Mr Iswaran to take a leave of absence until the investigation is completed. In his absence, Senior Minister of State Chee Hong Tat will be Acting Minister for Transport.Mr Iswaran is also minister-in-charge of trade relations at the Ministry of Trade and Industry (MTI). When contacted, a spokesperson said Mr Iswaran's portfolio at the ministry would be covered by other political officeholders during his leave of absence.\"As investigations are ongoing, we are unable to comment,\" said MTI.Other Members of Parliament from his constituency in West Coast will cover for Mr Iswaran as it will be \"difficult\" for him to undertake his duties as an MP, Deputy Prime Minister Lawrence Wong told reporters later on Wednesday.In its statement, CPIB said it will investigate this case \"thoroughly with strong resolve to establish the facts and the truth, and to uphold the rule of law\".\"CPIB acknowledges the interest by members of the public in this case because a minister is being interviewed by CPIB. As investigations are ongoing, CPIB is unable to provide further details.\"It also said Singapore has a strict zero-tolerance approach towards corruption.\"CPIB investigates all cases without fear or favour and will not hesitate to take action against any parties involved in corrupt activities,\" it added.Mr Iswaran’s political career spans more than 26 years since he was first elected in 1997 as a Member of Parliament for West Coast GRC.Before he was appointed to the Cabinet in 2006, he was on several government parliamentary committees and was the Deputy Speaker of Parliament from September 2004 to June 2006.He has been Minister for Transport since May 2021 and is concurrently Minister-in-charge of Trade Relations since May 2018.Mr Iswaran has also held ministerial positions in the Ministry of Communications and Information, the Ministry of Home Affairs and the Ministry of Education. He was also Minister in the Prime Minister's Office from May 2011 to September 2015.Before his career in politics, Mr Iswaran worked in both the public and private sector, including at the Ministry of Trade and Industry as well as Temasek Holdings. He joined the Singapore Administrative Service in 1987, serving in the Ministries of Home Affairs and Education before being seconded to the National Trades Union Congress. He was also the Singapore Indian Development Association’s first chief executive officer. Mr Iswaran read Economics at the University of Adelaide and graduated with First Class Honours. He also holds a Masters in Public Administration from Harvard University.\n",
      "['case', 'uncovered', 'anti', 'graft', 'agency', 'instructed', 'leave', 'absence', 'investigation', 'completed', 'assisting', 'investigation', 'case', 'uncovered', 'agency', 'bureau', 'elaborate', 'nature', 'investigation', 'statement', 'said', 'separate', 'statement', 'investigation', 'require', 'individual', 'interviewed', 'said', 'briefed', 'director', 'case', 'bureau', 'uncovered', 'director', 'sought', 'concurrence', 'open', 'formal', 'investigation', 'director', 'anti', 'graft', 'agency', 'sits', 'gave', 'concurrence', 'following', 'formal', 'investigation', 'began', 'said', 'said', 'instructed', 'leave', 'absence', 'investigation', 'completed', 'absence', 'minister', 'charge', 'trade', 'relation', 'contacted', 'spokesperson', 'said', 'portfolio', 'ministry', 'covered', 'political', 'officeholder', 'leave', 'absence', 'investigation', 'ongoing', 'unable', 'comment', 'said', 'constituency', 'cover', 'difficult', 'undertake', 'duty', 'told', 'reporter', 'later', 'statement', 'said', 'investigate', 'case', 'thoroughly', 'strong', 'resolve', 'establish', 'fact', 'truth', 'uphold', 'rule', 'acknowledges', 'interest', 'member', 'public', 'case', 'minister', 'interviewed', 'investigation', 'ongoing', 'unable', 'provide', 'detail', 'said', 'strict', 'tolerance', 'approach', 'corruption', 'investigates', 'case', 'fear', 'favour', 'hesitate', 'action', 'party', 'involved', 'corrupt', 'activity', 'added', 'political', 'career', 'spans', 'year', 'elected', 'grc.before', 'appointed', 'government', 'parliamentary', 'committee', 'concurrently', 'charge', 'held', 'ministerial', 'position', 'career', 'politic', 'worked', 'public', 'private', 'sector', 'including', 'joined', 'serving', 'seconded', 'chief', 'executive', 'officer', 'read', 'graduated', 'holds']\n",
      "Number of unique words: 156\n",
      "investigation: 9\n",
      "case: 6\n",
      "absence: 4\n",
      "agency: 3\n",
      "leave: 3\n",
      "statement: 3\n",
      "director: 3\n",
      "anti: 2\n",
      "graft: 2\n",
      "bureau: 2\n",
      "['investigation', 'case', 'absence', 'agency', 'leave', 'statement', 'director', 'anti', 'graft', 'bureau']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/singapore/iswaran-cpib-assist-investigations-ongoing-pm-lee-3622586\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7316844c",
   "metadata": {},
   "source": [
    "## 'Difficult' for Iswaran to undertake MP duties amid CPIB probe: Lawrence Wong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8d67113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      Singapore\n",
      "  \n",
      "Other West Coast GRC MPs will cover for him, says Deputy Prime Minister Lawrence Wong, who adds that this case is \"very different\" from the Ridout Road episode.SINGAPORE: Other Members of Parliament from West Coast will cover for Transport Minister S Iswaran as it will be \"difficult\" for him to undertake his duties while a Corrupt Practices Investigation Bureau (CPIB) probe is ongoing, said Deputy Prime Minister Lawrence Wong on Wednesday (Jul 12).Speaking to journalists, Mr Wong noted that since Mr Iswaran was instructed by Prime Minister Lee Hsien Loong to take a leave of absence, he would not be in a position to undertake his ministerial responsibilities.\"At the same time, it will be difficult for him also to undertake many of his MP duties, and so the other MPs from West Coast GRC will cover his MP responsibilities.\"CPIB on Wednesday morning said Mr Iswaran was assisting with its investigation into a case uncovered by the anti-graft agency. It did not elaborate on the nature of the investigation.Mr Lee said separately that the investigation would require Mr Iswaran, as well as other individuals, to be interviewed by CPIB.The Prime Minister also instructed Mr Iswaran to take a leave of absence until the investigation is completed. In his absence, Senior Minister of State Chee Hong Tat will be Acting Minister for Transport.Mr Iswaran is also minister-in-charge of trade relations at the Ministry of Trade and Industry (MTI). When contacted, a spokesperson said Mr Iswaran's portfolio at the ministry would be covered by other political officeholders during his leave of absence.\"As investigations are ongoing, we are unable to comment,\" said MTI.In a Facebook post on Wednesday evening, Minister for National Development Desmond Lee said he and the other West Coast GRC MPs will cover Mr Iswaran's duties, including meet-the-people sessions. \"We would like to assure residents of West Coast division that we will continue to serve them and ensure that their needs are supported,\" he wrote. Mr Wong also responded to questions on why Mr Iswaran was asked to take a leave of absence and why this was made public at this point, compared to what happened with CPIB's investigation into the rental of Ridout Road state properties by Law and Home Affairs Minister K Shanmugam and Foreign Affairs Minister Vivian Balakrishnan.Mr Wong acknowledged that Mr Iswaran's case was announced on the heels of the Ridout debate in parliament.\"But the two cases are very different,\" said the Deputy Prime Minister.In the case of Ridout, questions about the matter were raised in public and in parliament, Mr Wong noted.The two ministers also asked for an independent review of the matter, to which the Mr Lee agreed and asked CPIB to investigate the matter, he added.The investigations eventually concluded that there was no wrongdoing or corruption on the part of the ministers. The findings were published and there was a \"full accounting\" of the matter in parliament, said Mr Wong.In contrast, Mr Iswaran's \"case is completely driven by CPIB from the beginning\" and \"there was no public complaint\", said Mr Wong.\"It was CPIB that discovered the matter through their initial findings and investigations and they felt that there was a need to interview Minister Iswaran as part of further investigations.\"CPIB had been looking into an unrelated investigation on a separate matter earlier, and updated Mr Lee on this investigation in May, said Mr Wong, adding that he was also \"kept in the loop\" at the time.CPIB then continued their investigations and updated Mr Lee on their findings last week on Jul 5, and asked to interview Mr Iswaran.Within a day, Mr Lee agreed with the director of CPIB to open formal investigations, which began yesterday, added Mr Wong.\"I know Singaporeans are concerned and have many questions about this case. I'm unable to provide more information than what I've just said because the CPIB investigations are ongoing,\" he said.\"So I ask everyone that we allow the investigation to take its course and refrain from any further speculation at this juncture.\"Mr Wong said the CPIB investigation was \"concrete proof of how we do things in Singapore\".\"We have always upheld a clean and incorrupt system of government, and our track record on this over the decades is clear and evident to all,\" he said.\"And this is the foundation of the people's trust in the PAP (People's Action Party) government. The Prime Minister and I are fully committed to keeping and preserving this trust.\"The government will maintain a tough, zero-tolerance stance against corruption, and investigate cases that come up, said Mr Wong.\"And whichever way the facts eventually fall, they will be taken to their logical conclusion,\" he continued.\"We will be upfront and transparent, and we will not sweep anything under the carpet, even if they are potentially embarrassing or damaging to the PAP and to the government.\"\n",
      "['mps', 'cover', 'says', 'adds', 'case', 'different', 'episode', 'SINGAPORE', 'member', 'cover', 'difficult', 'undertake', 'duty', 'probe', 'ongoing', 'said', 'journalist', 'noted', 'instructed', 'leave', 'absence', 'position', 'undertake', 'ministerial', 'responsibility', '\"At', 'time', 'difficult', 'undertake', 'duty', 'mps', 'cover', 'responsibility', '\"cpib', 'morning', 'said', 'assisting', 'investigation', 'case', 'uncovered', 'anti', 'graft', 'agency', 'elaborate', 'nature', 'investigation', 'said', 'separately', 'investigation', 'require', 'individual', 'interviewed', 'cpib.the', 'instructed', 'leave', 'absence', 'investigation', 'completed', 'absence', 'minister', 'charge', 'trade', 'relation', 'contacted', 'spokesperson', 'said', 'portfolio', 'ministry', 'covered', 'political', 'officeholder', 'leave', 'absence', 'investigation', 'ongoing', 'unable', 'comment', 'said', 'post', 'evening', 'said', 'mps', 'cover', 'duty', 'including', 'meet', 'people', 'session', 'like', 'assure', 'resident', 'division', 'continue', 'serve', 'ensure', 'need', 'supported', 'wrote', 'responded', 'question', 'asked', 'leave', 'absence', 'public', 'point', 'compared', 'happened', 'investigation', 'rental', 'state', 'property', 'acknowledged', 'case', 'announced', 'heel', 'debate', 'parliament', 'case', 'different', 'said', 'case', 'question', 'matter', 'raised', 'public', 'parliament', 'noted', 'minister', 'asked', 'independent', 'review', 'matter', 'agreed', 'asked', 'investigate', 'matter', 'added', 'investigation', 'eventually', 'concluded', 'wrongdoing', 'corruption', 'minister', 'finding', 'published', 'accounting', 'matter', 'parliament', 'said', 'contrast', 'case', 'completely', 'driven', 'beginning', 'public', 'complaint', 'said', 'discovered', 'matter', 'initial', 'finding', 'investigation', 'felt', 'need', 'interview', 'investigation', 'looking', 'unrelated', 'investigation', 'separate', 'matter', 'early', 'updated', 'investigation', 'said', 'adding', 'kept', 'loop', 'time', 'cpib', 'continued', 'investigation', 'updated', 'finding', 'week', 'asked', 'interview', 'day', 'agreed', 'director', 'open', 'formal', 'investigation', 'began', 'yesterday', 'added', 'know', 'concerned', 'question', 'case', 'unable', 'provide', 'information', 'said', 'cpib', 'investigation', 'ongoing', 'said', 'ask', 'allow', 'investigation', 'course', 'refrain', 'speculation', 'juncture', 'said', 'investigation', 'concrete', 'proof', 'thing', 'upheld', 'clean', 'incorrupt', 'system', 'government', 'track', 'record', 'decade', 'clear', 'evident', 'said', 'foundation', 'people', 'trust', 'government', 'fully', 'committed', 'keeping', 'preserving', 'trust', 'government', 'maintain', 'tough', 'tolerance', 'stance', 'corruption', 'investigate', 'case', 'come', 'said', 'way', 'fact', 'eventually', 'fall', 'taken', 'logical', 'conclusion', 'continued', 'upfront', 'transparent', 'sweep', 'carpet', 'potentially', 'embarrassing', 'damaging', 'government']\n",
      "Number of unique words: 266\n",
      "investigation: 16\n",
      "case: 8\n",
      "matter: 6\n",
      "absence: 5\n",
      "leave: 4\n",
      "government: 4\n",
      "mps: 3\n",
      "duty: 3\n",
      "ongoing: 3\n",
      "minister: 3\n",
      "['investigation', 'case', 'matter', 'absence', 'leave', 'government', 'mps', 'duty', 'ongoing', 'minister']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/singapore/iswaran-cpib-investigation-leave-absence-mps-west-coast-lawrence-wong-3622871\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d224bea",
   "metadata": {},
   "source": [
    "## Proofer Bakery’s Changi City Point outlet suspended for two weeks due to infestation found on premises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "49f170be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "\n",
      "      Singapore\n",
      "  \n",
      "Proofer Bakery & Pizzeria at Changi City Point. (Photo: Google Maps)SINGAPORE: The licence of Proofer Bakery & Pizzeria’s outlet at Changi City Point has been suspended for two weeks due to its failure to keep its licensed premises free of infestation, said Singapore Food Agency (SFA) on Wednesday (Jul 12).The branch will not be allowed to operate from Jul 12 to Jul 25. Proofer was also fined S$800 (US$599) after accumulating 12 demerit points in a year. It was given six demerit points each for two offences of failing to keep licensed premises free of infestation. Under the SFA’s points system, a licensee who accumulates 12 or more demerit points during a 12-month period may have their licence suspended for a period of either two or four weeks, or cancelled.\"All food handlers working in the suspended premises would also be required to re-attend and pass the Food Safety Course Level 1, before they can resume work as food handlers,\" said the agency. The licensee is also required to ensure that all food hygiene officers working in the suspended premises, if any, re-attend and pass the Food Safety Course Level 3.SFA said it takes a serious view of these offences and reminded food operators to observe good food and personal hygiene practices at all times, and to engage only registered food handlers.The agency added it will not hesitate to take firm action against anyone found to be in violation of the Environmental Public Health Act.The central kitchen of Proofer Bakery was previously suspended after a “massive pest infestation” was detected by inspectors in October 2021. Food products distributed to Proofer’s retail outlets islandwide were also then recalled.In response to CNA's query, SFA said the suspension of Proofer's Changi City Point outlet is the first since the suspension of the central kitchen.Members of the public who come across poor food safety practices in food establishments are advised not to patronise the outlets. They can also provide feedback to SFA via its online feedback form or its contact centre at 6805 2871 with details for follow-up investigations. \n",
      "['photo', 'Maps)SINGAPORE', 'licence', 'outlet', 'suspended', 'week', 'failure', 'licensed', 'premise', 'free', 'infestation', 'said', 'branch', 'allowed', 'operate', 'fined', 's$800', 'accumulating', 'demerit', 'point', 'year', 'given', 'demerit', 'point', 'offence', 'failing', 'licensed', 'premise', 'free', 'infestation', 'point', 'system', 'licensee', 'accumulates', 'demerit', 'point', 'month', 'period', 'licence', 'suspended', 'period', 'week', 'cancelled', 'food', 'handler', 'working', 'suspended', 'premise', 'required', 'attend', 'pass', 'resume', 'work', 'food', 'handler', 'said', 'agency', 'licensee', 'required', 'ensure', 'food', 'hygiene', 'officer', 'working', 'suspended', 'premise', 'attend', 'pass', 'level', '3.sfa', 'said', 'takes', 'view', 'offence', 'reminded', 'food', 'operator', 'observe', 'good', 'food', 'personal', 'hygiene', 'practice', 'time', 'engage', 'registered', 'food', 'handler', 'agency', 'added', 'hesitate', 'firm', 'action', 'found', 'violation', 'central', 'kitchen', 'previously', 'suspended', 'massive', 'pest', 'infestation', 'detected', 'inspector', 'food', 'product', 'distributed', 'retail', 'outlet', 'islandwide', 'recalled', 'response', 'query', 'said', 'suspension', 'outlet', 'suspension', 'central', 'kitchen', 'member', 'public', 'come', 'poor', 'food', 'safety', 'practice', 'food', 'establishment', 'advised', 'patronise', 'outlet', 'provide', 'feedback', 'online', 'feedback', 'form', 'contact', 'centre', 'detail', 'follow', 'investigation']\n",
      "Number of unique words: 141\n",
      "food: 9\n",
      "outlet: 4\n",
      "premise: 4\n",
      "point: 4\n",
      "infestation: 3\n",
      "demerit: 3\n",
      "handler: 3\n",
      "licence: 2\n",
      "week: 2\n",
      "free: 2\n",
      "['food', 'outlet', 'premise', 'point', 'infestation', 'demerit', 'handler', 'licence', 'week', 'free']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "url_link = \"https://www.channelnewsasia.com/singapore/proofer-bakery-pizzeria-suspended-two-weeks-changi-city-point-sfa-infestation-licence-3624036\"\n",
    "article_text=\"\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "# Find the first parag with the content\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "paragraphs = main_body.find_all(\"p\")\n",
    "\n",
    "# Skip any empty paragraphs or paragraphs with specific class names, if necessary\n",
    "for paragraph in paragraphs:\n",
    "    if paragraph.text.strip() and \"class1\" not in paragraph.get(\"class\", []):\n",
    "        # Remove HTML tags,replace with spaces\n",
    "        cleaned_text = re.sub('<[^<]+?>', ' ', paragraph.text)\n",
    "        \n",
    "        # Remove extra whitespaces\n",
    "        cleaned_text = re.sub('\\s+', ' ', cleaned_text).strip()\n",
    "        \n",
    "        article_text += paragraph.text \n",
    "    else:\n",
    "        print(\"Article content not found.\")\n",
    "\n",
    "doc = nlp(article_text)\n",
    "print(doc)\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "        elif token.pos_ == 'VERB':\n",
    "            main_topic_words.append(token.text)\n",
    "print(main_topic_words)\n",
    "num_words = len(main_topic_words)\n",
    "print(f\"Number of unique words: {num_words}\")\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "import spacy\n",
    "\n",
    "result = []\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to get the words that are not part-of-speech or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in nlp.Defaults.stop_words and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "main_topic_words = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        if token.pos_ in ['NOUN', 'ADJ', 'ADV']:\n",
    "            main_topic_words.append(token.lemma_)\n",
    "\n",
    "output = get_hotwords(main_topic_words)\n",
    "word_counts = Counter(main_topic_words)\n",
    "\n",
    "most_common_list = word_counts.most_common(10)\n",
    "for item in most_common_list:\n",
    "    word, count = item\n",
    "    print(f\"{word}: {count}\")\n",
    "\n",
    "word_list = [word for word, _ in most_common_list]\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b2500",
   "metadata": {},
   "source": [
    "### Using Noun Chunk Extraction and Word Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e9e4e003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Ukraine: 15\n",
      "NATO: 14\n",
      "that: 10\n",
      "the alliance: 7\n",
      "Russia: 6\n",
      "they: 5\n",
      "Vilnius: 4\n",
      "Jul: 4\n",
      "Ms Rizzo: 4\n",
      "Tuesday: 4\n",
      "['Ukraine', 'NATO', 'that', 'the alliance', 'Russia', 'they', 'Vilnius', 'Jul', 'Ms Rizzo', 'Tuesday']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "from spacy.lang.en import STOP_WORDS\n",
    "\n",
    "# Function to get the words that are not stop words or punctuations\n",
    "def get_hotwords(words):\n",
    "    hotwords = []\n",
    "    for word in words:\n",
    "        if word.lower() not in STOP_WORDS and word not in punctuation and word.lower() != 'said' and word.lower() != 'say':\n",
    "            hotwords.append(word.lower())\n",
    "    return hotwords\n",
    "\n",
    "model = 'en_core_web_sm'\n",
    "spacy.cli.download(model)\n",
    "nlp = spacy.load(model)\n",
    "\n",
    "url_link = \"https://www.channelnewsasia.com/world/why-dont-some-nato-countries-want-ukraine-join-3623341\"\n",
    "result = requests.get(url_link).text\n",
    "soup = BeautifulSoup(result, \"html.parser\")\n",
    "\n",
    "# Find the main body of the article\n",
    "article_text = \"\"\n",
    "main_body = soup.find(\"div\", class_=\"content\")\n",
    "if main_body:\n",
    "    # Extract noun phrases as main topic words\n",
    "    doc = nlp(main_body.get_text())\n",
    "    main_topic_words = []\n",
    "    for chunk in doc.noun_chunks:\n",
    "        chunk_words = [token.text for token in chunk if not token.is_punct]\n",
    "        if chunk_words:\n",
    "            main_topic_words.append(\" \".join(chunk_words))\n",
    "    \n",
    "    output = get_hotwords(main_topic_words)\n",
    "    word_counts = Counter(main_topic_words)\n",
    "    \n",
    "    most_common_list = word_counts.most_common(10)\n",
    "    for item in most_common_list:\n",
    "        word, count = item\n",
    "        print(f\"{word}: {count}\")\n",
    "    \n",
    "    word_list = [word for word, _ in most_common_list]\n",
    "    print(word_list)\n",
    "else:\n",
    "    print(\"Article content not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db0251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
